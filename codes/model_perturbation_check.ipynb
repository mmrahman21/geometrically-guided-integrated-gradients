{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098a17b5",
   "metadata": {},
   "source": [
    "# Load all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b36049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixellib\n",
    "import cv2\n",
    "from cv2 import GaussianBlur\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import timeit\n",
    "from scipy import interpolate\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from sklearn import metrics\n",
    "from functools import partial\n",
    "\n",
    "from PIL import Image\n",
    "from methods.saliency_recent_real_metrics import add_random_pixels, interpolate_missing_pixels, \\\n",
    "                generate_saliency_focused_images_prev, generate_revised_saliency_focused_images, interpolate_img, calculate_webp_size\n",
    "\n",
    "from methods.method_research_utilities import load_cifar10_saliency_data, post_process_maps\n",
    "from methods.method_research_utilities import load_imagenet_saliency_data, load_imagenet_saliency_metric_eval_data, load_bgc_imagenet_saliency_data, load_cifar10_saliency_data, load_imgnet_val_data\n",
    "\n",
    "from methods.captum_post_process import _normalize_image_attr\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from methods.saliency_utilities import plot_maps_method_vertical, plot_maps_method_horizontal\n",
    "\n",
    "import io, os\n",
    "import skimage.io\n",
    "import skimage.filters\n",
    "from skimage import color\n",
    "\n",
    "from math import log, e\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "# Some plotting defaults\n",
    "sns.set_style('whitegrid', {'axes.grid': False})\n",
    "SSIZE=10\n",
    "MSIZE=12\n",
    "BSIZE=14\n",
    "plt.rc('font', size=SSIZE)\n",
    "plt.rc('axes', titlesize=MSIZE)\n",
    "plt.rc('axes', labelsize=MSIZE)\n",
    "plt.rc('xtick', labelsize=MSIZE)\n",
    "plt.rc('ytick', labelsize=MSIZE)\n",
    "plt.rc('legend', fontsize=MSIZE)\n",
    "plt.rc('figure', titlesize=MSIZE)\n",
    "plt.rcParams['font.family'] = \"sans-serif\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129a289",
   "metadata": {},
   "source": [
    "## Prepare the images for entropy calculation. \n",
    "\n",
    "It requires unnormalized and original images. This module properly resizes images to $224 \\times 224$ size and pixel values are kept in range $[0-255]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61df0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_unnormalized_images(images, target_labels):\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor()])\n",
    "\n",
    "    unnormalized_images = []\n",
    "    for image in images:\n",
    "        if isinstance(image, np.ndarray) and image.shape[0]==image.shape[1]:\n",
    "            img_tensor = torch.from_numpy(image)\n",
    "            img_tensor = img_tensor.permute(2, 0, 1)\n",
    "        else:\n",
    "            img_tensor = transform(image)\n",
    "            img_tensor = img_tensor*255\n",
    "            \n",
    "        img = img_tensor.to(int)\n",
    "        unnormalized_images.append(img)\n",
    "\n",
    "    unnormalized_images = torch.stack(unnormalized_images, dim=0)\n",
    "\n",
    "    unnormalized_img_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(unnormalized_images, target_labels), batch_size = unnormalized_images.shape[0], shuffle=False)\n",
    "    return unnormalized_images, unnormalized_img_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7837ffa4",
   "metadata": {},
   "source": [
    "# Normalize Images\n",
    "- argument images are already resized and within 0-255\n",
    "- output images are within 0-1 and z-scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images(images, target_labels, samples_per_batch=None):\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    normalized_images = []\n",
    "    labels = []\n",
    "    \n",
    "    for image, label in zip(images, target_labels):\n",
    "        img_tensor = transform(image)\n",
    "        normalized_images.append(img_tensor)\n",
    "        labels.append(label)\n",
    "        \n",
    "    normalized_images = torch.stack(normalized_images, dim=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    labels = labels.long()\n",
    "    \n",
    "    if samples_per_batch is None:\n",
    "        normalized_img_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(normalized_images, labels), batch_size = normalized_images.shape[0], shuffle=False)\n",
    "    \n",
    "    else:\n",
    "        normalized_img_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(normalized_images, labels), batch_size = samples_per_batch, shuffle=False)\n",
    "        \n",
    "    return normalized_images, normalized_img_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd169784",
   "metadata": {},
   "source": [
    "## Plot Images/Saliency Maps (for one method at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_or_maps(images, labels=None, categories=None, nrows=3, ncols=3, samples_to_show=list(range(100)), plot_type='images', save=False):\n",
    "    \n",
    "    \n",
    "    fig,axes=plt.subplots(nrows=nrows,ncols=ncols,figsize=(12,9),sharex=True,sharey=True)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        \n",
    "        img = images[samples_to_show[i]]\n",
    "        if not isinstance(img, np.ndarray):\n",
    "            img = img.numpy()\n",
    "        \n",
    "        if img.shape[0] <=3:\n",
    "            img = np.transpose(img, (1,2,0))\n",
    "            \n",
    "        if plot_type == 'images':\n",
    "            ax.imshow(img, interpolation=None, aspect='equal')\n",
    "        else:\n",
    "            ax.imshow(img, cmap='Reds', vmin=0, vmax=1)\n",
    "        \n",
    "        ax.axes.xaxis.set_ticks([])\n",
    "        ax.axes.yaxis.set_ticks([])\n",
    "\n",
    "        if labels is not None:\n",
    "            ax.set_title(categories[labels[i].item()], fontsize=6, y=0.95)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    plt.show()\n",
    "    \n",
    "    path = os.path.join('./Plots/Real/', \"method_research_\"+ plot_type)\n",
    "\n",
    "    if save:\n",
    "        fig.savefig(path+'.pdf', format='pdf', dpi=300)\n",
    "        #     fig.savefig(path+'.svg', transparent=True, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "        print('Plots Saved...', path)\n",
    "        \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c0598",
   "metadata": {},
   "source": [
    "## Load CAT vs DOG vs BIRD samples        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0debeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cat_dog_etc_data():\n",
    "    target = torch.as_tensor(13)\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    filename1 = './ImageNet/collection/my_junco.jpg'\n",
    "    filename2 = './ImageNet/collection/cat_dog_1.jpeg'\n",
    "    filename3 = './ImageNet/collection/cat_dog_2.jpeg'\n",
    "    \n",
    "    files = [filename1, filename2, filename3]\n",
    "    \n",
    "    for filename in files:\n",
    "\n",
    "        input_image = Image.open(filename)\n",
    "       \n",
    "        input_tensor = preprocess(input_image)\n",
    "        input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "        print(input_batch.shape)\n",
    "\n",
    "        data.append(input_tensor)\n",
    "        labels.append(target)\n",
    "\n",
    "    data = torch.stack(data, dim=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    labels = labels.long()\n",
    "    print(data.shape, labels.shape)\n",
    "\n",
    "    all_img_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(data, labels), batch_size = data.shape[0], shuffle=False)\n",
    "    \n",
    "    dataLoaderSal = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(data, labels), batch_size = 1, shuffle=False)\n",
    "\n",
    "    # Read the categories\n",
    "    with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "        classes = [s.strip() for s in f.readlines()]\n",
    "    \n",
    "    return data, labels, all_img_loader, dataLoaderSal, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae77b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_images, targets, all_img_loader, dataLoaderSal, categories = load_cat_dog_etc_data() \n",
    "# plot_images_or_maps(x_batch, labels=targets, categories=categories, nrows=1, ncols=2, samples_to_show=[0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47aade0",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d84adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(all_img_loader))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out, title=None)\n",
    "imshow(normalized_images[2])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb14e8",
   "metadata": {},
   "source": [
    "## Checking model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "\n",
    "name = 'inception_3'\n",
    "model = models.inception_v3(pretrained=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(normalized_images.shape)\n",
    "\n",
    "FinalData = normalized_images.to(device)\n",
    "Labels = targets.to(device)\n",
    "\n",
    "outputs = model(FinalData)\n",
    "_, preds = torch.max(outputs, 1)\n",
    "\n",
    "print(preds)\n",
    "\n",
    "print(preds.shape)\n",
    "\n",
    "for pred in preds:\n",
    "    pred = pred.item()\n",
    "\n",
    "    print(categories[pred])\n",
    "\n",
    "for idx, item in enumerate(categories):\n",
    "    if 'cat' in item.split(' '):\n",
    "        print(item, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf97933",
   "metadata": {},
   "source": [
    "## Test the unnormalized but resized image generation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac2a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "\n",
    "# Load Data\n",
    "images, normalized_2, target_labels, dataLoaderSal, categories = load_imgnet_val_data() \n",
    "\n",
    "# These are the images we used for perturbation test\n",
    "img_ids = [111, 114, 115, 122, 193]\n",
    "raw_imgs = []\n",
    "imgs = []\n",
    "targets = []\n",
    "\n",
    "for id in img_ids:\n",
    "    raw_imgs.append(images[id])\n",
    "    imgs.append(normalized_images[id])\n",
    "    targets.append(target_labels[id])\n",
    "    \n",
    "imgs = torch.stack(imgs, axis=0)\n",
    "targets = torch.stack(targets, axis=0)\n",
    "\n",
    "print('Dataset Shape {}, {}'.format(imgs.shape, targets.shape))\n",
    "\n",
    "unnormalized_images, unnormalized_img_loader = get_unnormalized_images(raw_imgs, targets)\n",
    "print(normalized_images.shape)\n",
    "print(unnormalized_images.shape)\n",
    "x_batch, y_batch = next(iter(unnormalized_img_loader))\n",
    "image_sample = x_batch[2].numpy()\n",
    "print(image_sample.shape)\n",
    "print(np.min(image_sample), np.max(image_sample))\n",
    "\n",
    "# plot_images_or_maps(x_batch, labels=targets, categories=categories, nrows=2, ncols=2, samples_to_show=[0, 2, 3, 4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a07368",
   "metadata": {},
   "source": [
    "#### Set the computational device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05eddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe7eb7",
   "metadata": {},
   "source": [
    "# Generate softmax scores\n",
    "\n",
    "This function is to generate softmax scores on the **original** or the **saliency-focused** images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08244925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_softmax_scores(model, normalized_images, pred_indices=None):\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        outputs = model(normalized_images)\n",
    "        scores = F.softmax(outputs, dim=1)\n",
    "\n",
    "        if pred_indices is None:\n",
    "            best_prob_scores, pred_indices = torch.max(scores, dim=1)\n",
    "        else:\n",
    "            best_prob_scores = scores.gather(1, pred_indices.view(-1,1))\n",
    "            best_prob_scores = torch.squeeze(best_prob_scores)\n",
    "                \n",
    "        return scores, best_prob_scores, pred_indices\n",
    "    \n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.data = X\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        return x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "def find_revised_softmax_scores(model, normalized_images, b_size=101, device=device):\n",
    "    \n",
    "    print(\"Original Data Shape:\", normalized_images.shape)\n",
    "    test_loader = torch.utils.data.DataLoader(MyDataset(normalized_images), batch_size=b_size, shuffle=False)\n",
    "    \n",
    "    all_scores = []\n",
    "    all_best_prob_scores = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (i, images) in enumerate(test_loader):\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print('Generating scores for {}-th batch'.format(i))\n",
    "                print('Image Shape: {}'.format(images.shape))\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            scores = F.softmax(outputs, dim=1).detach().cpu()\n",
    "            all_scores.append(scores)\n",
    "            \n",
    "            best_prob_scores, pred_indices = torch.max(scores, dim=1)\n",
    "            all_best_prob_scores.append(best_prob_scores)\n",
    "            all_predictions.append(pred_indices)\n",
    "            \n",
    "    all_scores = torch.stack(all_scores, axis=0)\n",
    "    all_scores = all_scores.squeeze(0)\n",
    "    all_best_prob_scores = torch.stack(all_best_prob_scores, axis=0)\n",
    "    all_best_prob_scores = all_best_prob_scores.squeeze(0)\n",
    "    all_predictions = torch.stack(all_predictions, axis=0)\n",
    "    all_predictions = all_predictions.squeeze(0)\n",
    "    \n",
    "    print(all_scores.shape)\n",
    "    print(all_best_prob_scores.shape)\n",
    "    print(all_predictions.shape)\n",
    "        \n",
    "    return all_scores.numpy(), all_best_prob_scores.numpy(), all_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343aaf3d",
   "metadata": {},
   "source": [
    "## Test the softmax score generation function as defined above\n",
    "\n",
    "- Define the model\n",
    "- Load the weights\n",
    "- Generate the images\n",
    "- Call the softmax score generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_models = {'Resnet18': models.resnet18, 'Resnet34': models.resnet34, 'resnet50': models.resnet50,\n",
    "                     'Resnet101': models.resnet101, 'inception_3': models.inception_v3}\n",
    "\n",
    "def load_pretrained_model(model_name):\n",
    "    \n",
    "    # Load the model\n",
    "    print(\"Loading pre-trained\", model_name, \"model\")\n",
    "    model = pretrained_models[model_name](pretrained=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0aec90",
   "metadata": {},
   "source": [
    "## post-process saliency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2f7ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = {0: 'mnist', 1: 'fmnist', 2: 'cifar10', 3: 'imgnet'}\n",
    "method_titles = [\"GD\", \"ONLY.IG\", \"ONLY.M\", \"GDAsc.IG\",\"GDAsc.M\", \"M.GDAsc.IG\", \"M.GDAsc.M\",\"Wt.P.IG\", \"Wt.P.M\", \"IG\", \"CaptIG\", \"L.IG\"]\n",
    "\n",
    "def post_process_saliency_maps(dataset_id, model_name=None, methods=None, saliency_path_prefix=None):\n",
    "    \n",
    "    name = 'inception_3' if model_name is None else model_name\n",
    "    methods_used = [\"GD\", \"ONLY.IG\", \"ONLY.M\", \"GDAsc\", \"M.GDAsc\", \"Wt.P\"] if methods is None else methods\n",
    "    fname_common = \"method_research_\"+Dataset[dataset_id]+\"_valSet_metricEval_\"+name if saliency_path_prefix is None else saliency_path_prefix\n",
    "\n",
    "    list_of_saliency_dicts, titles = post_process_maps(dataset_id, fname_common, method_list=methods_used,\\\n",
    "                                                          random_seeds=list(range(0, 1)), viz=False, scale_flag=False)\n",
    "    return list_of_saliency_dicts[0], titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a4058",
   "metadata": {},
   "source": [
    "### Post-process saliency maps for all perturbations and methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7f992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_list = [\"GD\", \"ONLY.IG\", \"ONLY.M\", \"M.GDAsc\"]\n",
    "name = 'Resnet101'\n",
    "\n",
    "all_randomizations_saliency_dict = {}\n",
    "for threshold in np.linspace(1, 100, 100):\n",
    "    prefix = 'method_research_'+name+'_perturbation_test_'+ str(int(threshold))\n",
    "\n",
    "    method_saliency_dict, title_set = post_process_saliency_maps(3, model_name=name, methods=method_list, saliency_path_prefix=prefix) # arg: dataset_id\n",
    "    all_randomizations_saliency_dict[int(threshold)] = method_saliency_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b2bc59",
   "metadata": {},
   "source": [
    "## Save and load processed saliency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5640eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def change_keys_and_save_saliency_dict(method_saliency_dict, title_set, fname=\"process_saliency_for_metricEval\"):\n",
    "    \n",
    "    saliency_dict_for_save = {}\n",
    "    for key1,key2 in zip(method_saliency_dict.keys(), title_set):\n",
    "        saliency_dict_for_save[key2] = method_saliency_dict[key1]\n",
    "\n",
    "    np.savez(fname+\".npz\", **saliency_dict_for_save)\n",
    "\n",
    "def load_saliency_dict_and_rename_keys(path=None):\n",
    "\n",
    "    saliency_dict = np.load(\"process_saliency_for_metricEval.npz\") if path is None else np.load(path)\n",
    "\n",
    "    new_saliency_dict = {}\n",
    "\n",
    "    for method_name, new_int_id in zip(saliency_dict.files, list(range(len(saliency_dict.files)))):\n",
    "        new_saliency_dict[new_int_id] = saliency_dict[method_name]\n",
    "    \n",
    "    return new_saliency_dict, saliency_dict.files\n",
    "\n",
    "# method_saliency_dict, title_set = load_saliency_dict_and_rename_keys()\n",
    "# print(method_saliency_dict.keys())\n",
    "# print(title_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3e34d2",
   "metadata": {},
   "source": [
    "## Do visualization of the comparable maps across several methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e5582",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "name = 'Resnet101_perturbation_test'\n",
    "\n",
    "def visualize_maps(name, display_range=[0, 10], f_size=(16, 16)):\n",
    "\n",
    "    # \"BlWhRd\"\n",
    "    colors = {'positive': 'Reds', 'absolute_value': 'Reds', 'all': LinearSegmentedColormap.from_list(\"RdWhGn\", [\"red\", \"white\",\"green\"])}\n",
    "    sign = 'absolute_value'\n",
    "\n",
    "    plot_maps_method_horizontal(model, normalized_images, target_labels, categories, name, \\\n",
    "                                'imgnet', method_saliency_dict, title_set, 0, \\\n",
    "                                range_to_display=np.asarray(range(display_range[0], display_range[1], 1)), fig_size=f_size, cm=colors[sign], vis_sign=sign)\n",
    "\n",
    "    plot_maps_method_vertical(model, normalized_images, target_labels, categories, name, \\\n",
    "                                'imgnet', method_saliency_dict, title_set, 0, \\\n",
    "                                range_to_display=np.asarray(range(display_range[0], display_range[1], 1)), fig_size=f_size, cm=colors[sign], vis_sign=sign)\n",
    "    \n",
    "\n",
    "\n",
    "# visualize_maps(name, display_range=[100, 110], f_size=(16, 16))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ae2e1",
   "metadata": {},
   "source": [
    "## Normalize saliency maps \n",
    "\n",
    "Provide the processed saliency maps (not normalized and not channel collapsed) of all methods in a dictionary structure. This method returns a new dictionary of normalized maps for all methods. \n",
    "\n",
    "- The output saliency maps do not have channel dimension (i.e. now it is 2D for images)\n",
    "- attribution values are within 0-1 for absolute masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d96a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_saliency_maps(saliency_method_dict, sign='absolute_value'):\n",
    "    saliency_maps_all_method = {} \n",
    "       \n",
    "    for method_name, all_saliency_images in saliency_method_dict.items():\n",
    "        normalized_saliency = []\n",
    "        for sal_image in all_saliency_images:\n",
    "            attribution = np.transpose(sal_image, (1,2,0))\n",
    "            norm_attr = _normalize_image_attr(attribution, sign)\n",
    "           \n",
    "            normalized_saliency.append(norm_attr)\n",
    "        \n",
    "        normalized_saliency = np.stack(normalized_saliency, axis=0)\n",
    "       \n",
    "        saliency_maps_all_method[method_name] = normalized_saliency\n",
    "    return saliency_maps_all_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title_set)\n",
    "normalized_saliency_maps_all_method = {}\n",
    "for threshold in np.linspace(1, 100, 100):\n",
    "    \n",
    "    method_saliency_dict = all_randomizations_saliency_dict[int(threshold)] \n",
    "    normalized_saliency_maps_all_method[int(threshold)] = normalize_saliency_maps(method_saliency_dict, sign='absolute_value')\n",
    "\n",
    "    change_keys_and_save_saliency_dict(normalized_saliency_maps_all_method[int(threshold)], title_set, fname=\"./MetricEvalEntropyMaps/normalized_saliency_perturbation_test_\"+str(int(threshold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561255f",
   "metadata": {},
   "source": [
    "### Load from disk if the normalized saliency maps are already saved there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a93960",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_maps_all_method = {}\n",
    "for threshold in np.linspace(1, 100, 100):\n",
    "    normalized_maps_all_method[int(threshold)], title_set = load_saliency_dict_and_rename_keys(path=\"./MetricEvalEntropyMaps/normalized_saliency_perturbation_test_\"+str(int(threshold))+\".npz\")\n",
    "    \n",
    "    if int(threshold) == 1:\n",
    "        print(normalized_maps_all_method[int(threshold)].keys())\n",
    "        print(title_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58720dd",
   "metadata": {},
   "source": [
    "## Plot the comparative mask and blurred images for a group of methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40fb2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparable_saliency_focused_mages(images, \n",
    "                                           original_saliency,\n",
    "                                           original_img_id,\n",
    "                                           saliency_all_methods,\n",
    "                                           method_ids,\n",
    "                                           method_names, \n",
    "                                           sample_id, \n",
    "                                           after=False, \n",
    "                                           save=False, \n",
    "                                           fname_hint=None, \n",
    "                                           fig_size=(12,9),\n",
    "                                           interp = 'none',\n",
    "                                           cm = 'afmhot',\n",
    "                                           vis_min = 0.0\n",
    "                                          ):\n",
    "    \n",
    "    ncols, nrows = 11, len(method_ids)\n",
    "    \n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    gs = gridspec.GridSpec(nrows, ncols,\n",
    "                       wspace=0.0, hspace=0.0)\n",
    "    \n",
    "    for i in range(nrows):\n",
    "        \n",
    "        img = images[sample_id].numpy()\n",
    "        \n",
    "        if np.squeeze(img).ndim == 3:\n",
    "            img = np.transpose(img, (1,2,0))\n",
    "        \n",
    "        ax = plt.subplot(gs[i, 0])\n",
    "        \n",
    "        ax.imshow(img, interpolation='none')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        if ax.get_subplotspec().is_first_row():\n",
    "                ax.set_title(\"Input\", fontsize=12)\n",
    "        \n",
    "    for i in range(nrows):\n",
    "        \n",
    "        main_image = images[sample_id].numpy()\n",
    "        main_image = np.transpose(main_image, (1,2,0))\n",
    "        \n",
    "        img_saliency_mask = original_saliency[i][original_img_id]\n",
    "        \n",
    "        new_mask = get_thresholded_saliency_mask_numpy(img_saliency_mask, 0.1)\n",
    "\n",
    "        # create 0/1 mask              \n",
    "        mask_3d = np.stack((new_mask,new_mask,new_mask),axis=2)\n",
    "\n",
    "        saliency_img = np.where(mask_3d==1, main_image, int(np.mean(main_image)))\n",
    "        saliency_img_only = mask_3d*main_image  # No interpolation on the updates\n",
    "\n",
    "        ax = plt.subplot(gs[i, 1])\n",
    "        \n",
    "        ax.imshow(saliency_img, interpolation=interp, vmin=vis_min, vmax=1.0, cmap=cm)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "        if ax.get_subplotspec().is_first_row():\n",
    "                ax.set_title(\"Original\\nExplanation\", fontsize=12)\n",
    "        \n",
    "    \n",
    "    # The first item in the method list is \"Input\", so start from the second \n",
    "    \n",
    "        \n",
    "    for i in range(ncols-2):\n",
    "        \n",
    "        perturbation_threshold_saliency_for_all_methods = saliency_all_methods[int(10*i+1)]\n",
    "        \n",
    "        for method_id, method_name in zip(range(len(method_ids[:])), method_names[1:]):\n",
    "\n",
    "            saliency = perturbation_threshold_saliency_for_all_methods[method_id]\n",
    "\n",
    "            main_image = images[sample_id].numpy()\n",
    "            main_image = np.transpose(main_image, (1,2,0))\n",
    "\n",
    "            img_saliency_mask = saliency[sample_id]\n",
    "\n",
    "            new_mask = get_thresholded_saliency_mask_numpy(img_saliency_mask, 0.1)\n",
    "\n",
    "            # create 0/1 mask              \n",
    "            mask_3d = np.stack((new_mask,new_mask,new_mask),axis=2)\n",
    "\n",
    "            saliency_img = np.where(mask_3d==1, main_image, int(np.mean(main_image)))\n",
    "            saliency_img_only = mask_3d*main_image  # No interpolation on the updates\n",
    "\n",
    "            ax = plt.subplot(gs[method_id, i+2])\n",
    "\n",
    "            ax.imshow(saliency_img, interpolation=interp, vmin=vis_min, vmax=1.0, cmap=cm)\n",
    "\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            if ax.get_subplotspec().is_first_row():\n",
    "                ax.set_title(\"{}%\".format(10*i + 1), fontsize=12)\n",
    "        \n",
    "    for method_id in range(nrows):\n",
    "        ax = plt.subplot(gs[method_id, 0])\n",
    "        print(method_names[method_id])\n",
    "        ax.set_ylabel(method_names[method_id+1], fontsize=12)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if save:\n",
    "        path = os.path.join('./Plots/Real/', fname_hint+\"_imageID_\"+str(original_img_id))\n",
    "#         fig.savefig(path+'.pdf', format='pdf', dpi=300)\n",
    "        fig.savefig(path+'.svg', transparent=True, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "        print('Plots Saved...', path)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268fc35",
   "metadata": {},
   "source": [
    "### Get only required saliency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_required_method_saliency(all_saliency_dict, method_list):\n",
    "    required_saliency_dict = {}\n",
    "    count = 0\n",
    "    for k, v in all_saliency_dict.items():\n",
    "        if k in method_list:\n",
    "            required_saliency_dict[count] = all_saliency_dict[k]\n",
    "            count += 1\n",
    "    \n",
    "    return required_saliency_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8fc64",
   "metadata": {},
   "source": [
    "### Load val images original saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dde7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_normalized_maps_all_method, title_loaded = load_saliency_dict_and_rename_keys(path=\"./MetricEvalEntropyMaps/normalized_saliency_for_valSet_metricEval_revised_with_edge_detector.npz\")\n",
    "print(val_normalized_maps_all_method.keys())\n",
    "print(title_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cde1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_required_saliency_dict = get_only_required_method_saliency(val_normalized_maps_all_method, [0, 1, 6])\n",
    "print(val_required_saliency_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d28dde",
   "metadata": {},
   "source": [
    "#### Visualizing model perturbation images across the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c308b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_thresholded_saliency_mask_numpy(saliency_mask, threshold_percent):\n",
    "    \n",
    "    thresholded_saliency_mask = np.zeros_like(saliency_mask.flatten())\n",
    "    no_of_required_salient_values = int(saliency_mask.flatten().shape[0]*threshold_percent)\n",
    "    \n",
    "    if no_of_required_salient_values > 0:\n",
    "        topk_salient_indices = np.argpartition(saliency_mask.flatten(), \\\n",
    "                                           -no_of_required_salient_values)[-no_of_required_salient_values:]\n",
    "        thresholded_saliency_mask[topk_salient_indices] = 1\n",
    "    \n",
    "    thresholded_saliency_mask = thresholded_saliency_mask.reshape(saliency_mask.shape)\n",
    "    \n",
    "    return thresholded_saliency_mask\n",
    "\n",
    "def get_masked_image(saliency_mask, main_img, threshold_p=0.1):\n",
    "    mask = get_thresholded_saliency_mask_numpy(saliency_mask, threshold_p)\n",
    "    mask_3d = np.stack((mask,mask,mask),axis=2)\n",
    "    saliency_img_only = mask_3d*main_img\n",
    "    \n",
    "    return saliency_img_only\n",
    "\n",
    "\n",
    "\n",
    "method_list = {0 : \"Gradients\", 1 : \"Integrated\\nGradients\", 2 : \"IG_Max\", \\\n",
    "               3 : \"GGIG_IG\", \\\n",
    "               4 : \"GGIG\"}\n",
    "\n",
    "desired_methods = [0, 1, 4]\n",
    "\n",
    "samples_to_show= [111, 114, 115, 122, 193]\n",
    "\n",
    "saliency_all_methods = {}\n",
    "method_captions = [\"Input\"]\n",
    "for sal_method_id in desired_methods:\n",
    "    \n",
    "    sal_method_new_name = method_list[sal_method_id]\n",
    "    \n",
    "    for threshold in np.linspace(1, 100, 100):\n",
    "        \n",
    "        all_image_saliency_masks = normalized_maps_all_method[int(threshold)]\n",
    "        saliency_all_methods[int(threshold)]= all_image_saliency_masks\n",
    "    method_captions.append(sal_method_new_name)\n",
    "\n",
    "# matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "\n",
    "for sample_id, original_img_id in enumerate(samples_to_show):\n",
    "    plot_comparable_saliency_focused_mages(x_batch, \n",
    "                                               val_required_saliency_dict,\n",
    "                                               original_img_id,\n",
    "                                               saliency_all_methods, \n",
    "                                               desired_methods, \n",
    "                                               method_captions,\n",
    "                                               sample_id, \n",
    "                                               after=False, \n",
    "                                               save=True, \n",
    "                                               fname_hint=\"perturbation_test\", \n",
    "                                               fig_size=(12,5),\n",
    "                                               interp = 'none',\n",
    "                                               cm = 'afmhot',\n",
    "                                               vis_min = 0.0\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9a521",
   "metadata": {},
   "source": [
    "### Plot the normalized softmax scores of the perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983cdc06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pallette_1 = ['#b2182b','#ef8a62','#fddbc7','#f7f7f7','#d1e5f0','#67a9cf','#2166ac']\n",
    "pallette_2 = ['#d73027','#f46d43','#fdae61','#fee090','#e0f3f8','#abd9e9','#74add1','#4575b4']\n",
    "pallette_3 = ['#8c510a','#bf812d','#2166ac','#80cdc1','#35978f','#01665e', '#4575b4']\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.0)\n",
    "img_id = 4\n",
    "basename = os.path.join('models and saliencies', 'saliency')\n",
    "\n",
    "used_legends = ['GRAD', 'IG', 'GGIG']\n",
    "\n",
    "for img_id in [0,1, 2, 3, 4]:\n",
    "    \n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "    for method in ['Grad', 'LocalIGAll', \"MultiGradAsc\"]:\n",
    "\n",
    "        score_path = os.path.join(basename, 'method_research_perturbation_test_all_scores_'+method+'_'+str(0)+'.npy')\n",
    "        comparative_prob_scores = np.load(score_path)\n",
    "        print(\"All comparative softmax scores saved here: {}\".format(score_path))\n",
    "        print(comparative_prob_scores.shape)\n",
    "\n",
    "        img_score = comparative_prob_scores[:, img_id] \n",
    "        img_score /= img_score[0]\n",
    "        img_score = np.clip(img_score, 0, 1)\n",
    "        plt.plot(np.linspace(0, 100, 101), img_score)\n",
    "\n",
    "    plt.legend(used_legends)\n",
    "    plt.xlabel(\"Perturbation Scale (%)\", fontsize=12)\n",
    "\n",
    "    y_label = \"Normalized Softmax Score\"\n",
    "    plt.ylabel(y_label, fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    path = os.path.join('./Plots/Real/', \"perturbation_test_\"+categories[targets[img_id]]+\"_softmax_scores\")\n",
    "    print(path)\n",
    "    fig.savefig(path+'.svg', transparent=True, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "    # fig.savefig(path+'.pdf', format='pdf', dpi=300)\n",
    "    print('Plots Saved...', path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
