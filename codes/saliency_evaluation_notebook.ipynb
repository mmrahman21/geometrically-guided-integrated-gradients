{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098a17b5",
   "metadata": {},
   "source": [
    "# Load all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b36049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixellib\n",
    "import cv2\n",
    "from cv2 import GaussianBlur\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import timeit\n",
    "from scipy import interpolate\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from sklearn import metrics\n",
    "from functools import partial\n",
    "\n",
    "from PIL import Image\n",
    "from methods.saliency_recent_real_metrics import add_random_pixels, interpolate_missing_pixels, \\\n",
    "                generate_saliency_focused_images_prev, generate_revised_saliency_focused_images, interpolate_img, calculate_webp_size, get_thresholded_saliency_mask_numpy\n",
    "\n",
    "from methods.method_research_utilities import load_cifar10_saliency_data, post_process_maps\n",
    "from methods.method_research_utilities import load_imagenet_saliency_data, load_imagenet_saliency_metric_eval_data, load_bgc_imagenet_saliency_data, load_cifar10_saliency_data, load_imgnet_val_data\n",
    "\n",
    "from methods.captum_post_process import _normalize_image_attr\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from methods.saliency_utilities import plot_maps_method_vertical, plot_maps_method_horizontal\n",
    "\n",
    "import io, os\n",
    "import skimage.io\n",
    "import skimage.filters\n",
    "from skimage import color\n",
    "\n",
    "from math import log, e\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "# Some plotting defaults\n",
    "sns.set_style('whitegrid', {'axes.grid': False})\n",
    "SSIZE=10\n",
    "MSIZE=12\n",
    "BSIZE=14\n",
    "plt.rc('font', size=SSIZE)\n",
    "plt.rc('axes', titlesize=MSIZE)\n",
    "plt.rc('axes', labelsize=MSIZE)\n",
    "plt.rc('xtick', labelsize=MSIZE)\n",
    "plt.rc('ytick', labelsize=MSIZE)\n",
    "plt.rc('legend', fontsize=MSIZE)\n",
    "plt.rc('figure', titlesize=MSIZE)\n",
    "plt.rcParams['font.family'] = \"sans-serif\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9231fc3",
   "metadata": {},
   "source": [
    "## Compute Sobel Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cdfe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sobel_edges(images):\n",
    "    # Read the original image\n",
    "\n",
    "    all_sobel_edges = []\n",
    "\n",
    "    for (idx, img) in enumerate(images):\n",
    "        \n",
    "        if isinstance(img, np.ndarray):\n",
    "            pass\n",
    "        else:\n",
    "            img = np.asarray(img).astype(np.uint8)\n",
    "\n",
    "        # Blur the image for better edge detection\n",
    "        img_blur = cv2.GaussianBlur(img, (3, 3), sigmaX=0, sigmaY=0)\n",
    "       \n",
    "        # Sobel Edge Detection\n",
    "\n",
    "        sobelxy = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=1, dy=1, ksize=5)  # Combined X and Y Sobel Edge Detection\n",
    "\n",
    "        all_sobel_edges.append(sobelxy)\n",
    "\n",
    "    all_sobel_edges = np.stack(all_sobel_edges, axis=0)\n",
    "\n",
    "    \n",
    "    return all_sobel_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77cd100",
   "metadata": {},
   "source": [
    "## Build Entropy-Softmax Interpolation Module\n",
    "\n",
    "This module does interpolation to get common entropy-softmax values for all images. These common x-axis values will help to aggregate the result for the final PIC (SIC and AIC) plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41701db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_entropy_vs_scores(relative_entropy, scores):\n",
    "\n",
    "    f = interpolate.interp1d(relative_entropy, scores, fill_value=\"extrapolate\")\n",
    "    xnew = np.linspace(0.0, 1.0, 101)\n",
    "    ynew = f(xnew) \n",
    "    return xnew, ynew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feefc4a0",
   "metadata": {},
   "source": [
    "## Test the interpolation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d72d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.0, 1.0, 10)\n",
    "y = np.exp(-x/3.0)\n",
    "print(interpolate_entropy_vs_scores(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129a289",
   "metadata": {},
   "source": [
    "## Prepare the images for entropy calculation. \n",
    "\n",
    "It requires unnormalized and original images. This module properly resizes images to $224 \\times 224$ size and pixel values are kept in range $[0-255]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61df0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_unnormalized_images(images, target_labels):\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor()])\n",
    "\n",
    "    unnormalized_images = []\n",
    "    for image in images:\n",
    "        if isinstance(image, np.ndarray) and image.shape[0]==image.shape[1]:\n",
    "            img_tensor = torch.from_numpy(image)\n",
    "            img_tensor = img_tensor.permute(2, 0, 1)\n",
    "        else:\n",
    "            img_tensor = transform(image)\n",
    "            img_tensor = img_tensor*255\n",
    "            \n",
    "        img = img_tensor.to(int)\n",
    "        unnormalized_images.append(img)\n",
    "\n",
    "    unnormalized_images = torch.stack(unnormalized_images, dim=0)\n",
    "\n",
    "    unnormalized_img_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(unnormalized_images, target_labels), batch_size = unnormalized_images.shape[0], shuffle=False)\n",
    "    return unnormalized_images, unnormalized_img_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7837ffa4",
   "metadata": {},
   "source": [
    "# Normalize Images\n",
    "- argument images are already resized and within 0-255\n",
    "- output images are within 0-1 and z-scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images(images, target_labels, samples_per_batch=None):\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    normalized_images = []\n",
    "    labels = []\n",
    "    \n",
    "    for image, label in zip(images, target_labels):\n",
    "        img_tensor = transform(image)\n",
    "        normalized_images.append(img_tensor)\n",
    "        labels.append(label)\n",
    "        \n",
    "    normalized_images = torch.stack(normalized_images, dim=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    labels = labels.long()\n",
    "    \n",
    "    if samples_per_batch is None:\n",
    "        normalized_img_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(normalized_images, labels), batch_size = normalized_images.shape[0], shuffle=False)\n",
    "    \n",
    "    else:\n",
    "        normalized_img_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(normalized_images, labels), batch_size = samples_per_batch, shuffle=False)\n",
    "        \n",
    "    return normalized_images, normalized_img_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd169784",
   "metadata": {},
   "source": [
    "## Plot Images/Saliency Maps (for one method at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_or_maps(images, labels=None, categories=None, nrows=3, ncols=3, samples_to_show=list(range(100)), plot_type='images', cm='bwr', vis_min=0.0, save=False):\n",
    "    \n",
    "    \n",
    "    fig,axes=plt.subplots(nrows=nrows,ncols=ncols,figsize=(9,9),sharex=True,sharey=True)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        \n",
    "        img = images[samples_to_show[i]]\n",
    "        if not isinstance(img, np.ndarray):\n",
    "            img = img.numpy()\n",
    "        \n",
    "        if img.shape[0] <=3:\n",
    "            img = np.transpose(img, (1,2,0))\n",
    "            \n",
    "        if plot_type == 'images':\n",
    "            ax.imshow(img)\n",
    "        else:\n",
    "            ax.imshow(img, cmap=cm, interpolation='none', vmin=vis_min, vmax=1.0)\n",
    "        \n",
    "        ax.axes.xaxis.set_ticks([])\n",
    "        ax.axes.yaxis.set_ticks([])\n",
    "\n",
    "        if labels is not None:\n",
    "            ax.set_title(categories[labels[i].item()], fontsize=6, pad=5)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    plt.show()\n",
    "    \n",
    "    path = os.path.join('./Plots/Real/', \"method_research_\"+ plot_type+'_'+cm)\n",
    "\n",
    "    if save:\n",
    "        fig.savefig(path+'.pdf', format='pdf', dpi=300)\n",
    "        #     fig.savefig(path+'.svg', transparent=True, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "        print('Plots Saved...', path)\n",
    "        \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf97933",
   "metadata": {},
   "source": [
    "## Test the unnormalized but resized image generation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac2a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, normalized_images, target_labels, dataLoaderSal, categories = load_imgnet_val_data() # load_imagenet_saliency_metric_eval_data()\n",
    "\n",
    "\n",
    "unnormalized_images, unnormalized_img_loader = get_unnormalized_images(images, target_labels)\n",
    "print(normalized_images.shape)\n",
    "print(unnormalized_images.shape)\n",
    "x_batch, y_batch = next(iter(unnormalized_img_loader))\n",
    "image_sample = np.transpose(x_batch[101].numpy(), (1, 2, 0))\n",
    "print(image_sample.shape)\n",
    "print(np.min(image_sample), np.max(image_sample))\n",
    "\n",
    "# plt.imshow(image_sample)\n",
    "\n",
    "plot_images_or_maps(x_batch, labels=target_labels[100:], categories=categories, nrows=10, ncols=10, samples_to_show=list(range(100,200)))\n",
    "# unnormalized_images = unnormalized_images.permute((0, 2, 3, 1))\n",
    "# sobel_edges = compute_sobel_edges(unnormalized_images)\n",
    "# sobel_edges = np.moveaxis(sobel_edges, 3, 1)\n",
    "\n",
    "# print(sobel_edges.shape)\n",
    "# print(np.min(sobel_edges[0]), np.max(sobel_edges[0]))\n",
    "# plot_images_or_maps(sobel_edges, labels=target_labels, categories=categories, nrows=10, ncols=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea5ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "0, 2, 7, 9,18, 20,25,34,37, 38, 45, 80, 82, 83, 101, 123, 179, 193"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a07368",
   "metadata": {},
   "source": [
    "#### Set the computational device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05eddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe7eb7",
   "metadata": {},
   "source": [
    "# Generate softmax scores\n",
    "\n",
    "This function is to generate softmax scores on the **original** or the **saliency-focused** images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08244925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_softmax_scores(model, normalized_images, pred_indices=None):\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        outputs = model(normalized_images)\n",
    "        scores = F.softmax(outputs, dim=1)\n",
    "\n",
    "        if pred_indices is None:\n",
    "            best_prob_scores, pred_indices = torch.max(scores, dim=1)\n",
    "        else:\n",
    "            best_prob_scores = scores.gather(1, pred_indices.view(-1,1))\n",
    "            best_prob_scores = torch.squeeze(best_prob_scores)\n",
    "                \n",
    "        return scores, best_prob_scores, pred_indices\n",
    "    \n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.data = X\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        return x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "def find_revised_softmax_scores(model, normalized_images, b_size=101, device=device):\n",
    "    \n",
    "    print(\"Original Data Shape:\", normalized_images.shape)\n",
    "    test_loader = torch.utils.data.DataLoader(MyDataset(normalized_images), batch_size=b_size, shuffle=False)\n",
    "    \n",
    "    all_scores = []\n",
    "    all_best_prob_scores = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (i, images) in enumerate(test_loader):\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print('Generating scores for {}-th batch'.format(i))\n",
    "                print('Image Shape: {}'.format(images.shape))\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            scores = F.softmax(outputs, dim=1).detach().cpu()\n",
    "            all_scores.append(scores)\n",
    "            \n",
    "            best_prob_scores, pred_indices = torch.max(scores, dim=1)\n",
    "            all_best_prob_scores.append(best_prob_scores)\n",
    "            all_predictions.append(pred_indices)\n",
    "            \n",
    "    all_scores = torch.stack(all_scores, axis=0)\n",
    "    all_scores = all_scores.squeeze(0)\n",
    "    all_best_prob_scores = torch.stack(all_best_prob_scores, axis=0)\n",
    "    all_best_prob_scores = all_best_prob_scores.squeeze(0)\n",
    "    all_predictions = torch.stack(all_predictions, axis=0)\n",
    "    all_predictions = all_predictions.squeeze(0)\n",
    "    \n",
    "    print(all_scores.shape)\n",
    "    print(all_best_prob_scores.shape)\n",
    "    print(all_predictions.shape)\n",
    "        \n",
    "    return all_scores.numpy(), all_best_prob_scores.numpy(), all_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343aaf3d",
   "metadata": {},
   "source": [
    "## Test the softmax score generation function as defined above\n",
    "\n",
    "- Define the model\n",
    "- Load the weights\n",
    "- Generate the images\n",
    "- Call the softmax score generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_models = {'Resnet18': models.resnet18, 'Resnet34': models.resnet34, 'resnet50': models.resnet50,\n",
    "                     'Resnet101': models.resnet101, 'inception_3': models.inception_v3}\n",
    "\n",
    "def load_pretrained_model(model_name):\n",
    "    \n",
    "    # Load the model\n",
    "    print(\"Loading pre-trained\", model_name, \"model\")\n",
    "    model = pretrained_models[model_name](pretrained=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images\n",
    "images, normalized_images, target_labels, dataLoaderSal, categories = load_imagenet_saliency_metric_eval_data() #load_imgnet_val_data() #\n",
    "unnormalized_images, unnormalized_img_loader = get_unnormalized_images(images, target_labels)\n",
    "\n",
    "normalized_images = normalized_images.to(device)\n",
    "target_labels =target_labels.to(device)\n",
    "\n",
    "# Call the model loader\n",
    "model = load_pretrained_model('inception_3')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Call the softmax function to generate scores\n",
    "pred_indices = None\n",
    "scores, best_prob_scores, pred_indices = find_softmax_scores(model, normalized_images, pred_indices=pred_indices)\n",
    "print(categories[pred_indices[83].item()])\n",
    "# print(scores.shape)\n",
    "# print(best_prob_scores, pred_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0aec90",
   "metadata": {},
   "source": [
    "## post-process saliency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2f7ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = {0: 'mnist', 1: 'fmnist', 2: 'cifar10', 3: 'imgnet'}\n",
    "method_titles = [\"GD\", \"ONLY.IG\", \"ONLY.M\", \"GDAsc.IG\",\"GDAsc.M\", \"M.GDAsc.IG\", \"M.GDAsc.M\",\"Wt.P.IG\", \"Wt.P.M\", \"IG\", \"CaptIG\", \"L.IG\"]\n",
    "\n",
    "def post_process_saliency_maps(dataset_id, model_name=None, methods=None, saliency_path_prefix=None):\n",
    "    \n",
    "    name = 'inception_3' if model_name is None else model_name\n",
    "    methods_used = [\"GD\", \"ONLY.IG\", \"ONLY.M\", \"GDAsc\", \"M.GDAsc\", \"Wt.P\"] if methods is None else methods\n",
    "    fname_common = \"method_research_\"+Dataset[dataset_id]+\"_valSet_metricEval_\"+name if saliency_path_prefix is None else saliency_path_prefix\n",
    "\n",
    "    list_of_saliency_dicts, titles = post_process_maps(dataset_id, fname_common, method_list=methods_used,\\\n",
    "                                                          random_seeds=list(range(0, 1)), viz=False, scale_flag=False)\n",
    "    return list_of_saliency_dicts[0], titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7f992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_saliency_dict, title_set = post_process_saliency_maps(3, model_name='Resnet101') # arg: dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b2bc59",
   "metadata": {},
   "source": [
    "## Save and load processed saliency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5640eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def change_keys_and_save_saliency_dict(method_saliency_dict, title_set, fname=\"process_saliency_for_metricEval\"):\n",
    "    \n",
    "    saliency_dict_for_save = {}\n",
    "    for key1,key2 in zip(method_saliency_dict.keys(), title_set):\n",
    "        saliency_dict_for_save[key2] = method_saliency_dict[key1]\n",
    "\n",
    "    np.savez(fname+\".npz\", **saliency_dict_for_save)\n",
    "\n",
    "def load_saliency_dict_and_rename_keys(path=None):\n",
    "\n",
    "    saliency_dict = np.load(\"process_saliency_for_metricEval.npz\") if path is None else np.load(path)\n",
    "\n",
    "    new_saliency_dict = {}\n",
    "\n",
    "    for method_name, new_int_id in zip(saliency_dict.files, list(range(len(saliency_dict.files)))):\n",
    "        new_saliency_dict[new_int_id] = saliency_dict[method_name]\n",
    "    \n",
    "    return new_saliency_dict, saliency_dict.files\n",
    "\n",
    "# method_saliency_dict, title_set = load_saliency_dict_and_rename_keys()\n",
    "# print(method_saliency_dict.keys())\n",
    "# print(title_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3e34d2",
   "metadata": {},
   "source": [
    "## Do visualization of the comparable maps across several methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e5582",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "name = 'Resnet101_valSet_MetricEval'\n",
    "\n",
    "def visualize_maps(name, display_range=[0, 10], f_size=(16, 16)):\n",
    "\n",
    "    # \"BlWhRd\"\n",
    "    colors = {'positive': 'Reds', 'absolute_value': 'Reds', 'all': LinearSegmentedColormap.from_list(\"RdWhGn\", [\"red\", \"white\",\"green\"])}\n",
    "    sign = 'absolute_value'\n",
    "\n",
    "    plot_maps_method_horizontal(model, normalized_images, target_labels, categories, name, \\\n",
    "                                'imgnet', method_saliency_dict, title_set, 0, \\\n",
    "                                range_to_display=np.asarray(range(display_range[0], display_range[1], 1)), fig_size=f_size, cm=colors[sign], vis_sign=sign)\n",
    "\n",
    "    plot_maps_method_vertical(model, normalized_images, target_labels, categories, name, \\\n",
    "                                'imgnet', method_saliency_dict, title_set, 0, \\\n",
    "                                range_to_display=np.asarray(range(display_range[0], display_range[1], 1)), fig_size=f_size, cm=colors[sign], vis_sign=sign)\n",
    "    \n",
    "\n",
    "\n",
    "# visualize_maps(name, display_range=[100, 110], f_size=(16, 16))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ae2e1",
   "metadata": {},
   "source": [
    "## Normalize saliency maps \n",
    "\n",
    "Provide the processed saliency maps (not normalized and not channel collapsed) of all methods in a dictionary structure. This method returns a new dictionary of normalized maps for all methods. \n",
    "\n",
    "- The output saliency maps do not have channel dimension (i.e. now it is 2D for images)\n",
    "- attribution values are within 0-1 for absolute masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d96a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_saliency_maps(saliency_method_dict, sign='absolute_value'):\n",
    "    saliency_maps_all_method = {} \n",
    "       \n",
    "    for method_name, all_saliency_images in saliency_method_dict.items():\n",
    "        normalized_saliency = []\n",
    "        for sal_image in all_saliency_images:\n",
    "            attribution = np.transpose(sal_image, (1,2,0))\n",
    "            norm_attr = _normalize_image_attr(attribution, sign)\n",
    "           \n",
    "            normalized_saliency.append(norm_attr)\n",
    "        \n",
    "        normalized_saliency = np.stack(normalized_saliency, axis=0)\n",
    "       \n",
    "        saliency_maps_all_method[method_name] = normalized_saliency\n",
    "    return saliency_maps_all_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_saliency_maps_all_method = normalize_saliency_maps(method_saliency_dict, sign='absolute_value')\n",
    "# print(normalized_saliency_maps_all_method[0].shape)\n",
    "# print(len(normalized_saliency_maps_all_method))\n",
    "\n",
    "sal_dict = {'edge': sobel_edges}\n",
    "print(sobel_edges.shape)\n",
    "normalized_edge_detector = normalize_saliency_maps(sal_dict, sign='absolute_value')\n",
    "print(normalized_edge_detector['edge'].shape)\n",
    "print(np.min(normalized_edge_detector['edge'][0]), np.max(normalized_edge_detector['edge'][0]))\n",
    "plot_images_or_maps(normalized_edge_detector['edge'], categories=categories, nrows=10, ncols=10, plot_type=\"metricEval_edge_detector\", save=True)\n",
    "normalized_saliency_maps_all_method[9] = normalized_edge_detector['edge']\n",
    "title_set.append(\"Edge Detector\")\n",
    "change_keys_and_save_saliency_dict(normalized_saliency_maps_all_method, title_set, fname=\"./MetricEvalEntropyMaps/normalized_saliency_for_valSet_metricEval_revised_with_edge_detector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561255f",
   "metadata": {},
   "source": [
    "### Load from disk if the normalized saliency maps are already saved there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a93960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ResNet101 valSet maps, use path=\"./MetricEvalEntropyMaps/normalized_saliency_for_valSet_metricEval_revised_with_edge_detector.npz\"\n",
    "normalized_maps_all_method, title_set = load_saliency_dict_and_rename_keys(path=\"./MetricEvalEntropyMaps/normalized_saliency_for_metricEval_revised_with_edge_detector.npz\")\n",
    "print(normalized_maps_all_method.keys())\n",
    "print(title_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c4f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_required_method_saliency(all_saliency_dict, method_list):\n",
    "    required_saliency_dict = {}\n",
    "    count = 0\n",
    "    for k, v in all_saliency_dict.items():\n",
    "        if k in method_list:\n",
    "            required_saliency_dict[count] = all_saliency_dict[k]\n",
    "            count += 1\n",
    "    \n",
    "    return required_saliency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb39d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['Input', 'Gradients', \"Integrated\\nGradients\", \"GGIG\", \"Edge\\nDetector\"]\n",
    "required_saliency_dict = get_only_required_method_saliency(normalized_maps_all_method, [0, 1, 6, 9])\n",
    "print(required_saliency_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747ab450",
   "metadata": {},
   "source": [
    "### Comparable visualization of normalized saliency masks across methods\n",
    "- Vertical plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1c3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_maps_method_vertical_testcode(\n",
    "        images,\n",
    "        saliency_dict, \n",
    "        method_captions, \n",
    "        samples_to_show = np.asarray(range(95, 105, 1)), \n",
    "        fig_size = (10, 5),\n",
    "        cm=None, \n",
    "        interp = 'none',\n",
    "        vis_min = 0.0, \n",
    "        save = False,\n",
    "        fname=\"my_file\"\n",
    "    ):\n",
    "    saliency_methods_total_id = len(saliency_dict)\n",
    "    \n",
    "    nrows = saliency_methods_total_id+1\n",
    "    ncols = len(samples_to_show)\n",
    "    \n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    gs = gridspec.GridSpec(nrows, ncols,\n",
    "                       wspace=0.0, hspace=0.0)\n",
    "\n",
    "    for i in range(ncols):\n",
    "        \n",
    "        img = images[samples_to_show[i]].numpy()\n",
    "        \n",
    "        if np.squeeze(img).ndim == 3:\n",
    "            img = np.transpose(img, (1,2,0))\n",
    "        \n",
    "        ax = plt.subplot(gs[0, i])\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "\n",
    "    for method_id in range(saliency_methods_total_id):\n",
    "\n",
    "        saliency = saliency_dict[method_id]\n",
    "\n",
    "        for i in range(ncols):\n",
    "\n",
    "            sample = saliency_dict[method_id][samples_to_show[i]]\n",
    "            ax = plt.subplot(gs[method_id + 1, i])\n",
    "\n",
    "            ax.imshow(sample, interpolation = interp,\n",
    "                          vmin=vis_min,\n",
    "                          vmax=1.0,\n",
    "                          cmap=cm)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "\n",
    "    for method_id in range(saliency_methods_total_id + 1):\n",
    "        ax = plt.subplot(gs[method_id, 0])\n",
    "        ax.set_ylabel(method_captions[method_id], fontsize=9)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    if save:\n",
    "        path = os.path.join('./Plots/Real/', fname+'_'+cm)\n",
    "        fig.savefig(path+'.svg', transparent=True, bbox_inches='tight', pad_inches=0)\n",
    "#         fig.savefig(path+'.pdf', format='pdf', dpi=300)\n",
    "        print('Plots Saved...', path)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a37903",
   "metadata": {},
   "source": [
    "- Horizontal plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparable_saliency_maps(images, \n",
    "                                       normalized_maps_all_method, \n",
    "                                       method_names, \n",
    "                                       samples_to_show=list(range(10)), \n",
    "                                       fig_size = (11, 10),\n",
    "                                       cm='bwr', vis_min=0.0, save=False, fname=None):\n",
    "    \n",
    "    ncols, nrows = len(normalized_maps_all_method), len(samples_to_show)\n",
    "    \n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    gs = gridspec.GridSpec(nrows, ncols+1,\n",
    "                       wspace=0.0, hspace=0.0)\n",
    "    \n",
    "    for row in range(nrows):\n",
    "        \n",
    "        img = images[samples_to_show[row]].numpy()\n",
    "        \n",
    "        if img.shape[0] <=3:\n",
    "            img = np.transpose(img, (1,2,0))\n",
    "            \n",
    "        ax = plt.subplot(gs[row, 0])\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    for method_id in range(ncols):\n",
    "\n",
    "        saliency = normalized_maps_all_method[method_id]\n",
    "\n",
    "        for i in range(nrows):\n",
    "\n",
    "            sample = normalized_maps_all_method[method_id][samples_to_show[i]]\n",
    "            \n",
    "            if sample.shape[0] <= 3:\n",
    "                sample = np.transpose(sample, (1, 2, 0))\n",
    "                \n",
    "            ax = plt.subplot(gs[i, method_id + 1])\n",
    "\n",
    "            ax.imshow(sample,\n",
    "                          vmin=vis_min,\n",
    "                          vmax=1.0,\n",
    "                          cmap=cm)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            \n",
    "            if ax.get_subplotspec().is_first_row():\n",
    "                ax.set_title(method_names[method_id], fontsize=9)  #axes[0, method_id + 1]\n",
    "    \n",
    "    ax = plt.subplot(gs[0, 0])\n",
    "    ax.set_title('Input', fontsize=9)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    if save:\n",
    "        path = os.path.join('./Plots/Real/', fname)\n",
    "        fig.savefig(path+'.pdf', format='pdf', dpi=300)\n",
    "        #     fig.savefig(path+'.svg', transparent=True, bbox_inches='tight', pad_inches=0)\n",
    "        print('Plots Saved...', path)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a81918",
   "metadata": {},
   "source": [
    "### Call saliency plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9c895",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "method_names = titles\n",
    "cmap = 'bwr_r'\n",
    "vmin = -1.0\n",
    "\n",
    "cm_vs_min = {'bwr': -1.0, 'bwr_r': -1.0, 'coolwarm': -1.0, 'Reds': 0.0, 'gray': 0.0, 'inferno': 0.0, 'afmhot': 0.0}\n",
    "\n",
    "for cm_value, min_value in cm_vs_min.items():\n",
    "    \n",
    "    plot_maps_method_vertical_testcode(x_batch, \n",
    "                                required_saliency_dict, \n",
    "                                method_names, \n",
    "                                samples_to_show=[2, 18,37, 45, 80, 82, 83, 101, 124, 179, 193], \n",
    "                                fig_size=(11, 5), \n",
    "                                cm=cm_value, \n",
    "                                interp = 'none',\n",
    "                                vis_min= min_value, \n",
    "                                save=True,\n",
    "                                fname = \"inception_testSet_interpolation_none_viz\"\n",
    "                               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58720dd",
   "metadata": {},
   "source": [
    "## Plot the comparative mask and blurred images for a group of methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40fb2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparable_saliency_focused_mages(images, \n",
    "                                           saliency_all_methods, \n",
    "                                           method_names, \n",
    "                                           samples_to_show = np.asarray(range(10)), \n",
    "                                           threshold_percent = 10,\n",
    "                                           after=False, \n",
    "                                           save=False, \n",
    "                                           fname_hint=None, \n",
    "                                           fig_size=(12,9)\n",
    "                                          ):\n",
    "    \n",
    "    '''\n",
    "    Input: \n",
    "    - all images\n",
    "    - all method saliencies for all methods\n",
    "    - a set of samples identifications\n",
    "    - a threshold value (percentage)\n",
    "    \n",
    "    Output:\n",
    "    plot a comparative saliency focused images for the given samples\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    saliency_methods_total_id = len(saliency_all_methods)\n",
    "    \n",
    "    nrows = saliency_methods_total_id+1\n",
    "    ncols = len(samples_to_show)\n",
    "    \n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    gs = gridspec.GridSpec(nrows, ncols,\n",
    "                       wspace=0.0, hspace=0.0)\n",
    "\n",
    "    for i in range(ncols):\n",
    "        \n",
    "        img = images[samples_to_show[i]].numpy()\n",
    "        \n",
    "        if np.squeeze(img).ndim == 3:\n",
    "            img = np.transpose(img, (1,2,0))\n",
    "        \n",
    "        ax = plt.subplot(gs[0, i])\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "    # The first item in the method list is \"Input\", so start from the second \n",
    "    for method_id, method_name in zip(range(len(method_names[1:])), method_names[1:]):\n",
    "        \n",
    "        saliency = saliency_all_methods[method_name]\n",
    "       \n",
    "        for i in range(ncols):\n",
    "            \n",
    "            main_image = images[samples_to_show[i]].numpy()\n",
    "            main_image = np.transpose(main_image, (1,2,0))\n",
    "            \n",
    "            img_saliency_mask = saliency[samples_to_show[i]]\n",
    "            \n",
    "            new_mask = get_thresholded_saliency_mask_numpy(img_saliency_mask, threshold_percent)\n",
    "\n",
    "            # create 0/1 mask              \n",
    "            mask_3d = np.stack((new_mask,new_mask,new_mask),axis=2)\n",
    "\n",
    "            saliency_img = np.where(mask_3d==1, main_image, int(np.mean(main_image)))\n",
    "            saliency_img_only = mask_3d*main_image  # No interpolation on the updates\n",
    "            \n",
    "            ax = plt.subplot(gs[method_id + 1, i])\n",
    "\n",
    "            ax.imshow(saliency_img)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "    for method_id in range(saliency_methods_total_id + 1):\n",
    "        ax = plt.subplot(gs[method_id, 0])\n",
    "        ax.set_ylabel(method_names[method_id], fontsize=9)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    if save:\n",
    "        path = os.path.join('./Plots/Real/', fname_hint+'_threshold_p_'+str(int(threshold_percent*100)))\n",
    "        fig.savefig(path+'.pdf', format='pdf', dpi=300)\n",
    "        fig.savefig(path+'.svg', transparent=True, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "        print('Plots Saved...', path)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0622ec20",
   "metadata": {},
   "source": [
    "## Same sample, multiple threshold, all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f50bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_saliency_focus_at_multiple_threshold(images, \n",
    "                                           saliency_all_methods, \n",
    "                                           method_names, \n",
    "                                           sample_to_show = 0, \n",
    "                                           threshold_values = [3, 5, 7, 10],\n",
    "                                           after=False, \n",
    "                                           save=False, \n",
    "                                           fname_hint=None, \n",
    "                                           fig_size=(12,9)\n",
    "                                          ):\n",
    "    \n",
    "    saliency_methods_total_id = len(saliency_all_methods)\n",
    "    \n",
    "    nrows = len(threshold_values)+1\n",
    "    ncols = saliency_methods_total_id\n",
    "    \n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    gs = gridspec.GridSpec(nrows, ncols,\n",
    "                       wspace=0.0, hspace=0.0)\n",
    "\n",
    "    for i in range(ncols):\n",
    "        \n",
    "        img = images[sample_to_show].numpy()\n",
    "        \n",
    "        if np.squeeze(img).ndim == 3:\n",
    "            img = np.transpose(img, (1,2,0))\n",
    "        \n",
    "        ax = plt.subplot(gs[0, i])\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "    # The first item in the method list is \"Input\", so start from the second \n",
    "    for method_id, method_name in zip(range(len(method_names[1:])), method_names[1:]):\n",
    "        \n",
    "        saliency = saliency_all_methods[method_name]\n",
    "       \n",
    "        for i in range(1, nrows):\n",
    "            \n",
    "            main_image = images[sample_to_show].numpy()\n",
    "            main_image = np.transpose(main_image, (1,2,0))\n",
    "            \n",
    "            img_saliency_mask = saliency[sample_to_show]\n",
    "            \n",
    "            new_mask = get_thresholded_saliency_mask_numpy(img_saliency_mask, threshold_values[i-1])\n",
    "\n",
    "            # create 0/1 mask              \n",
    "            mask_3d = np.stack((new_mask,new_mask,new_mask),axis=2)\n",
    "\n",
    "            saliency_img = np.where(mask_3d==1, main_image, int(np.mean(main_image)))\n",
    "            saliency_img_only = mask_3d*main_image  # No interpolation on the updates\n",
    "            \n",
    "            ax = plt.subplot(gs[i, method_id])\n",
    "\n",
    "            ax.imshow(saliency_img)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            \n",
    "            if method_id == 0:\n",
    "                ax.set_ylabel(str(int(threshold_values[i-1]*100))+\"%\", fontsize=12)\n",
    "\n",
    "    for method_id in range(saliency_methods_total_id):\n",
    "        ax = plt.subplot(gs[0, method_id])\n",
    "        ax.set_title(method_names[method_id+1], fontsize=12)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    if save:\n",
    "        path = os.path.join('./Plots/Real/', fname_hint+'_multiple_threshold_for_img_'+str(sample_to_show))\n",
    "#         fig.savefig(path+'.pdf', format='pdf', dpi=300)\n",
    "        fig.savefig(path+'.svg', transparent=True, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "        print('Plots Saved...', path)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e9a8c",
   "metadata": {},
   "source": [
    "## Plot Relative Entropy vs. Threshold Information\n",
    "\n",
    "- $x$ axis implies threshold (x% pixels added to the image)\n",
    "- $y$ asis gives relative entropy (entropy of linearly interpolated saliency image w.r.t original image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f39e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entropy_vs_threshold(percent_vs_entropy):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(percent_vs_entropy[0], percent_vs_entropy[1])\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel('Threshold %')\n",
    "    ax.set_title('Threshold vs Entropy')\n",
    "    ax.set_yticks(np.linspace(0.2, 1.0, num=9))\n",
    "    ax.set_yticks(np.linspace(0.2, 1.0, num=9))\n",
    "    ax.set_ylabel('Relative Entropy')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d91564",
   "metadata": {},
   "source": [
    "## Create random image with 1% pixels and generate mask for absent pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc58be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_img_and_mask_for_interpolation(numpy_image, initial_percentage=0.01):\n",
    "    random_img, mask = add_random_pixels(numpy_image, p=initial_percentage)\n",
    "    print(random_img.shape)\n",
    "    print(np.count_nonzero(mask))\n",
    "    mask_rand = (ma.array(mask[:, :]) == 0).data\n",
    "    print(\"Mask Shape:\", mask_rand.shape)\n",
    "    return random_img, mask_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a596af",
   "metadata": {},
   "source": [
    "## Compute relative entropy (used for primary checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf80c0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numpy_image = np.transpose(x_batch[193].numpy(), (1,2,0))\n",
    "print(numpy_image.shape)\n",
    "print(np.min(numpy_image), np.max(numpy_image))\n",
    "\n",
    "main_size = calculate_webp_size(numpy_image)\n",
    "\n",
    "sal_mask = normalized_maps_all_method[6][122]\n",
    "\n",
    "# Create random image and interpolation mask mask\n",
    "random_img, mask_rand = create_random_img_and_mask_for_interpolation(numpy_image, initial_percentage=0.005)\n",
    "random_interpolated_img = interpolate_img(random_img, mask_rand, interp_mode='nearest')\n",
    "plt.imshow(random_interpolated_img)\n",
    "\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "# saliency_images, pixel_percent_n_entropy = generate_revised_saliency_focused_images(numpy_image, random_img, mask_rand, random_interpolated_img, \\\n",
    "#                              iterations=101, saliency_mask=sal_mask, interp_mode = 'nearest')\n",
    "\n",
    "# print(saliency_images.shape)\n",
    "# print(pixel_percent_n_entropy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00afff8d",
   "metadata": {},
   "source": [
    "# Create Evaluation Pipeline\n",
    "- Plot how to generate test image for softmax information curve (SIC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_pipeline(images,\n",
    "                             saliency_all_methods, \n",
    "                             method_name, \n",
    "                             img_id, \n",
    "                             threshold_percent = 10,\n",
    "                             save=False, \n",
    "                             fname_hint=None, \n",
    "                             fig_size=(12,1),\n",
    "                             cm='afmhot',\n",
    "                             vis_min=0.0\n",
    "                            ):\n",
    "\n",
    "    nrows, ncols = 1, 5\n",
    "    \n",
    "    captions = [\"Main\\nImage\", \"GGIG\\nExplanation\", \"Defocused\\nImage\", \"Thresholded\\nSaliency\", \"Test\\nImage\" ]\n",
    "    \n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    gs = gridspec.GridSpec(nrows, ncols,\n",
    "                       wspace=0.0, hspace=0.0)\n",
    "    \n",
    "    # Prepare main image\n",
    "    \n",
    "    img = images[img_id].numpy()\n",
    "    print('Main Img:', img.shape)\n",
    "\n",
    "    if np.squeeze(img).ndim == 3:\n",
    "        img = np.transpose(img, (1,2,0))\n",
    "        \n",
    "    # Prepare random initial interpolated image\n",
    "    \n",
    "    random_img, mask_rand = create_random_img_and_mask_for_interpolation(img, initial_percentage=0.005)\n",
    "    rand_interpolated_img = interpolate_img(random_img, mask_rand, interp_mode='nearest')\n",
    "    \n",
    "    # Prepare the thresholded saliency mask\n",
    "    \n",
    "    saliency = saliency_all_methods[method_name]       \n",
    "    img_saliency_mask = saliency[img_id]\n",
    "    new_mask = get_thresholded_saliency_mask_numpy(img_saliency_mask, threshold_percent)\n",
    "    \n",
    "    # create 0/1 mask \n",
    "    initial_random_mask = 1 - (1*mask_rand)\n",
    "    mask = 1*((new_mask == 1) | (initial_random_mask == 1))                \n",
    "    mask_3d = np.stack((mask,mask,mask),axis=2)    \n",
    "    mask_3d_no_random = np.stack((new_mask,new_mask,new_mask),axis=2)\n",
    "\n",
    "    # Generate thresholded mask and test image\n",
    "    saliency_mask_img = np.where(mask_3d_no_random==1, img, int(np.mean(img)))\n",
    "    test_img = np.where(mask_3d==1, img, rand_interpolated_img)\n",
    "#     saliency_img_only = mask_3d*main_image  # No interpolation on the updates\n",
    "    \n",
    "    for i in range(ncols):\n",
    "        ax = plt.subplot(gs[0, i])\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.imshow(img) \n",
    "         \n",
    "        elif i == 1:\n",
    "            ax.imshow(img_saliency_mask, interpolation='none', cmap=cm, vmin=vis_min, vmax=1.0)\n",
    "            \n",
    "        elif i == 2:\n",
    "            ax.imshow(rand_interpolated_img)\n",
    "            \n",
    "        elif i == 3:\n",
    "            ax.imshow(saliency_mask_img)\n",
    "            \n",
    "        else:\n",
    "            ax.imshow(test_img)\n",
    "            \n",
    "        ax.set_title(captions[i], fontsize=12, pad=5)\n",
    "            \n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    if save:\n",
    "        path = os.path.join('./Plots/Real/', fname_hint+'_eval_pipeline_threshold_p_'+str(int(threshold_percent*100))+'_img_'+str(img_id))\n",
    "#         fig.savefig(path+'.pdf', format='pdf', dpi=300)\n",
    "        fig.savefig(path+'.svg', transparent=True, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "        print('Plots Saved...', path)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce0a8a",
   "metadata": {},
   "source": [
    "## Execute evaluation\n",
    "1. Start with an empty image (e.g. black image). Let's call it image A.\n",
    "2. Add 1% of random pixels from the original image to image A.\n",
    "3. According to a given saliency method, add top x% of the most important pixels from the original image to image A, where x% is a chosen threshold.\n",
    "4. Apply linear interpolation (or nearest neighbors) to image A to find the value of pixels that were not added in step 2 or 3.\n",
    "5. Compute model output on image A (let's call the value T). Compute model output on the original image (let's call the value U). Compute T/U and clip it to [0, 1]. Sometimes the intermediate image A can be assigned a higher score than the original image, thus unclipped T/U can be higher than 1.0. This step gives the y-axis value.\n",
    "6. Compute the amount of information in image A (I) and the original image (J) using WebP. Compute I/J and clip it to [0, 1]. This will give you the x-axis value.\n",
    "    \n",
    "    $blurred\\_image\\_rel\\_entropy = \\frac{size\\_of\\_the\\_blurred\\_image \\, - \\, size\\_of\\_the\\_random\\_image}{size\\_of\\_the\\_original\\_image \\, - \\, size\\_of\\_the\\_random\\_image}$ \n",
    "    \n",
    "    <code>Note from Author: Sorry, that was my mistake. The relative entropy value is calculated as (\"image entropy\" - \"completely blurred image entropy\") / (\"original image entropy\" - \"completely blurred image entropy\"). Thus, the saliency threshold 0% corresponds to a relative value equal to 0.</code>\n",
    "    \n",
    "    \n",
    "7. Apply steps 3 to 6 on different thresholds (x% in step 3).\n",
    "8. Apply steps 1 to 7 on different images and aggregate the result.\n",
    "\n",
    "   <code>Note from Author: When computing the entropy and softmax we use 100 steps based on image area e.g. start with 1% of image -> 2% -> 3% until 100%. For each image we interpolate the normalized softmax to normalized entropy points between 0.0 and 1.0 (in x axis, you can use scipy interpolate.interp1d for this). For example points (entropy, softmax) = (0.1, 0.1), (0.5, 0.5), (1.0, 0.6) would produce a line with slope of 1 till 0.5 and slope of 0.2 afterwards. Then we take the median of these curves in y-axis over all the images. We use 100 interpolation points between 0.0 and 1.0 so that the x points are aligned across examples before we take the median in y-axis.</code> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586f6714",
   "metadata": {},
   "source": [
    "#### **Step A:** Define a function to create blurred images for different threshold (x%) for all images, based on a saliency method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12510018",
   "metadata": {},
   "source": [
    "#### Step A (Revision): to create blurred images \n",
    "\n",
    "- Use nearest neighbor interpolation once!\n",
    "- No interpolation after adding salient pixels\n",
    "- No use of compression package (main trouble). Rather directly use entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38420c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each image, compute different blurred images, relevant entropy values per threshold (x%)\n",
    "# Provide \n",
    "\n",
    "def create_new_set_of_blurred_images(all_images, all_images_saliency, sal_method_name):\n",
    "    \n",
    "    all_blurred_images = {}\n",
    "    all_relative_entropy = {}\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    for i, image in enumerate(all_images):\n",
    "        print('Image Count: {}'.format(i))\n",
    "        numpy_image = np.transpose(image.numpy(), (1,2,0))\n",
    "        print(numpy_image.shape)\n",
    "        print(np.min(numpy_image), np.max(numpy_image))\n",
    "\n",
    "        # Create random image and interpolation mask mask\n",
    "        random_img, mask_rand = create_random_img_and_mask_for_interpolation(numpy_image, initial_percentage=0.005)\n",
    "        \n",
    "        random_interpolated_img = interpolate_img(random_img, mask_rand, interp_mode='nearest')\n",
    "\n",
    "        saliency_images, pixel_percent_n_entropy = generate_revised_saliency_focused_images(numpy_image, random_img, mask_rand, random_interpolated_img, \\\n",
    "                             iterations=101, saliency_mask=all_images_saliency[i], interp_mode = 'nearest')\n",
    "\n",
    "        print(saliency_images.shape)\n",
    "        \n",
    "        print(pixel_percent_n_entropy.shape)\n",
    "        all_blurred_images[str(i)] = saliency_images\n",
    "        all_relative_entropy[str(i)] = pixel_percent_n_entropy\n",
    "        elapsed_time = time.time() - since\n",
    "        print('Total Time Elapsed:', elapsed_time)\n",
    "    np.savez(\"./MetricEvalEntropyMaps/all_blurred_images_for_metricEval_May1_half_percent_\"+sal_method_name+\".npz\", **all_blurred_images)\n",
    "    np.savez(\"./MetricEvalEntropyMaps/all_blurred_images_rel_entropy_for_metricEval_May1_half_percent_\"+sal_method_name+\".npz\", **all_relative_entropy)\n",
    "    print(datetime.now())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3081e6",
   "metadata": {},
   "source": [
    "#### Step B:  Use the function defined above to create all sets of blurred images per saliency method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da4b8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "method_id = 10 #int(sys.argv[1])\n",
    "if method_id <= 9:\n",
    "    all_images_saliency = normalized_maps_all_method[method_id]\n",
    "    sal_method_name = title_set[method_id]\n",
    "    print(\"Saliency Shape: {}\".format(all_images_saliency.shape))\n",
    "\n",
    "else: \n",
    "    all_images_saliency = [None]*len(x_batch)\n",
    "    sal_method_name = \"Random\"\n",
    "    print(\"This saliency does not have shape..It is all None...\")\n",
    "    \n",
    "print(\"Saliency Method Name: {}\".format(sal_method_name))\n",
    "create_new_set_of_blurred_images(x_batch, all_images_saliency, sal_method_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d28dde",
   "metadata": {},
   "source": [
    "#### Step C (To Check): Visualizing the created blurred images and entropy trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23af13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "method_list = {'GD': \"Gradients\", 'ONLY.IG': \"Integrated\\nGradients\", 'ONLY.M': \"IG_Max\", \\\n",
    "               'GDAsc.IG': \"GDA_IG\", 'GDAsc.M': \"GDA_Max\", 'M.GDAsc.IG': \"GGIG_IG\", \\\n",
    "               'M.GDAsc.M': \"GGIG\", 'Wt.P.IG': \"GEG_IG\", 'Wt.P.M': \"GEG_Max\", \\\n",
    "               'Random': \"Random\", \"Edge Detector\": \"Edge\\nDetector\"}\n",
    "\n",
    "desired_methods = [0, 1, 2, 3]\n",
    "\n",
    "method_id_method_name_dict = {0: 'Gradients', 1: 'Integrated\\nGradients', 2: 'GGIG', 3: 'Edge\\nDetector'}\n",
    "samples_to_show= [24,37, 45, 82, 83, 124]\n",
    "\n",
    "saliency_all_methods = {}\n",
    "methods = [\"Input\"]\n",
    "for sal_method_id in desired_methods:\n",
    "    \n",
    "    sal_method_new_name = method_id_method_name_dict[sal_method_id]\n",
    "    \n",
    "    all_images_saliency = required_saliency_dict[sal_method_id]\n",
    "\n",
    "    saliency_all_methods[sal_method_new_name]=all_images_saliency\n",
    "    methods.append(sal_method_new_name)\n",
    "\n",
    "# for p in [0.03, 0.05, 0.07, 0.10]:\n",
    "    \n",
    "#     plot_comparable_saliency_focused_mages(x_batch, \n",
    "#                                                saliency_all_methods, \n",
    "#                                                methods, \n",
    "#                                                samples_to_show = samples_to_show, \n",
    "#                                                threshold_percent = p,\n",
    "#                                                after=False, \n",
    "#                                                save=True, \n",
    "#                                                fname_hint=\"inception_testSet_compare_saliency_focused_images\", \n",
    "#                                                fig_size=(8,7)\n",
    "#                                               )\n",
    "\n",
    "for sample in samples_to_show:\n",
    "    \n",
    "    plot_saliency_focus_at_multiple_threshold(x_batch, \n",
    "                                               saliency_all_methods, \n",
    "                                               methods, \n",
    "                                               sample_to_show = sample, \n",
    "                                               threshold_values = [0.03, 0.05, 0.07, 0.10],\n",
    "                                               after=False, \n",
    "                                               save=True, \n",
    "                                               fname_hint=\"inception_testSet_illustrative_example\", \n",
    "                                               fig_size=(7,9)\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d15c0a4",
   "metadata": {},
   "source": [
    "### Generate Evaluation Pipeline Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9393286",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# [0, 1, 2, 3]\n",
    "saliency_method_id = 2\n",
    "\n",
    "method_id_method_name_dict = {0: 'Gradients', 1: 'Integrated\\nGradients', 2: 'GGIG', 3: 'Edge\\nDetector'}\n",
    "samples_to_show=[89,111, 114, 115, 119, 122, 193]\n",
    "\n",
    "for img_id in samples_to_show:\n",
    "    \n",
    "    saliency_all_methods = {}\n",
    "\n",
    "    sal_method_new_name = method_id_method_name_dict[saliency_method_id]\n",
    "\n",
    "    all_images_saliency = required_saliency_dict[saliency_method_id]\n",
    "\n",
    "    saliency_all_methods[sal_method_new_name]=all_images_saliency\n",
    "\n",
    "    plot_evaluation_pipeline(x_batch,\n",
    "                             saliency_all_methods, \n",
    "                             sal_method_new_name, \n",
    "                             img_id, \n",
    "                             threshold_percent = 0.1,\n",
    "                             save=True, \n",
    "                             fname_hint=\"resnet101_valSet_eval_pipeline\", \n",
    "                             fig_size=(12,2.5), \n",
    "                             cm='afmhot',\n",
    "                             vis_min=0.0\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837ef89a",
   "metadata": {},
   "source": [
    "## Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a3866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(outputs, y):\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    accuracy = (preds == y).sum().item()\n",
    "    accuracy /= y.size(0)\n",
    "    return accuracy\n",
    "\n",
    "def compute_auc_for_pic_metric(x, y):\n",
    "    \n",
    "    measure_a = np.round(np.mean((y[:-1] + y[1:])/2.0, 0), 3)\n",
    "    measure_b = np.round(metrics.auc(x, y), 3)\n",
    "    assert measure_a == measure_b, \"Both computations should produce same results.\"\n",
    "    \n",
    "    return measure_a\n",
    "\n",
    "\n",
    "def prepare_dataframe_for_plot(all_method_result, legends):\n",
    "    '''\n",
    "    all_method_result should be in (samples, methods, interpolation_points) shape\n",
    "    samples: how many images/samples\n",
    "    methods: how many interpretability methods\n",
    "    legends: name of the interpretability methods\n",
    "    '''\n",
    "    \n",
    "    all_result_data_frames=[]\n",
    "\n",
    "    for (idx, title) in enumerate(legends):\n",
    "        \n",
    "        print(all_method_result.shape)\n",
    "        \n",
    "        if idx == 7:\n",
    "            print(all_method_result[:, idx, 100])\n",
    "        \n",
    "        per_method_intermediate_results = all_method_result[:, idx, 0::10]\n",
    "        print(per_method_intermediate_results.shape)\n",
    "        \n",
    "        df = pd.DataFrame(per_method_intermediate_results)\n",
    "        \n",
    "        df.columns = np.linspace(0, 10, 11).astype(int).tolist()\n",
    "        \n",
    "        for time_interval in df.columns:\n",
    "            per_interval_result = df[[time_interval]]\n",
    "            per_interval_result.columns = [\"Score\"]\n",
    "            per_interval_result.insert(0, \"Threshold %\", 10*time_interval)\n",
    "            per_interval_result.insert(2, 'Method', title)\n",
    "            all_result_data_frames.append(per_interval_result)\n",
    "\n",
    "    FinalResult = pd.concat(all_result_data_frames, axis=0)\n",
    "    FinalResult.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return FinalResult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8dae83",
   "metadata": {},
   "source": [
    "#### **Step D :** Feed the blurred images per saliency method to produce scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scores_on_saliency_focused_images(model, main_resized_unnormalized_images, main_labels, blurred_images, relative_entropies):\n",
    "    print(main_resized_unnormalized_images.shape)\n",
    "    \n",
    "    main_images, main_img_loader = normalize_images(main_resized_unnormalized_images, main_labels)\n",
    "    \n",
    "    # Call the softmax function to generate scores\n",
    "    all_scores, main_images_prob_scores, main_pred_indices = find_revised_softmax_scores(model, main_images, b_size=main_images.shape[0], device=device)\n",
    "    \n",
    "    all_images_all_interpolation = []\n",
    "    all_images_all_relative_entropy = []\n",
    "    \n",
    "    for subj_id in range(main_images.shape[0]):\n",
    "        \n",
    "        if subj_id % 25 == 0:\n",
    "            print(\"Working on Subj: {}\".format(subj_id))\n",
    "        subj_blurred_images = blurred_images[str(subj_id)]\n",
    "        \n",
    "        subj_blurred_images_after_interpolation = subj_blurred_images[:, 1, :, :, :]\n",
    "#         print(subj_blurred_images_after_interpolation.shape)\n",
    "#         subj_blurred_images_after_interpolation = np.moveaxis(subj_blurred_images_after_interpolation, 1, -1)\n",
    "        pixel_percent_rel_entropies = relative_entropies[str(subj_id)]\n",
    "        subj_blurred_images_after_interpolation = subj_blurred_images_after_interpolation.astype(np.uint8)\n",
    "        entropies = pixel_percent_rel_entropies[1, :]\n",
    "        \n",
    "        subj_blur_images, subj_blur_img_loader = normalize_images(subj_blurred_images_after_interpolation, [main_pred_indices[subj_id]]*101)\n",
    "        all_images_all_interpolation.append(subj_blur_images)\n",
    "        all_images_all_relative_entropy.append(entropies)\n",
    "\n",
    "    all_images_all_interpolation = torch.stack(all_images_all_interpolation, axis=0)\n",
    "    all_images_all_relative_entropy = np.stack(all_images_all_relative_entropy, axis=0)\n",
    "    print(all_images_all_interpolation.shape)\n",
    "    print(all_images_all_relative_entropy.shape)\n",
    "    all_images_all_interpolation = all_images_all_interpolation.reshape(-1, *all_images_all_interpolation.shape[2:]) \n",
    "    all_blur_images_all_prob_scores, best_blur_scores, blur_pred_indices = find_revised_softmax_scores(model, all_images_all_interpolation, b_size=101, device=device)\n",
    "\n",
    "    return main_images_prob_scores, main_pred_indices, all_blur_images_all_prob_scores, best_blur_scores, blur_pred_indices, all_images_all_relative_entropy\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1acbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_set.append('Random') # Add this to the title set\n",
    "print(title_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd6937a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_results_dictionary(main_image_batch, sal_method_name, model_name, blur_image_path=None, blur_image_rel_entropy_path=None):\n",
    "    \n",
    "    print(\"Saliency Method Name: {}\".format(sal_method_name))\n",
    "\n",
    "    if blur_image_path is None:\n",
    "        blurred_images_before_n_after_interp = np.load(\"./MetricEvalEntropyMaps/all_blurred_images_for_metricEval_May1_half_percent_\"+sal_method_name+\".npz\")\n",
    "        blurred_images_rel_entropy = np.load(\"./MetricEvalEntropyMaps/all_blurred_images_rel_entropy_for_metricEval_May1_half_percent_\"+sal_method_name+\".npz\")\n",
    "\n",
    "    else:\n",
    "        blurred_images_before_n_after_interp = np.load(blur_image_path+sal_method_name+\".npz\")\n",
    "        blurred_images_rel_entropy = np.load(blur_image_rel_entropy_path+sal_method_name+\".npz\")\n",
    "\n",
    "    # Call the model loader\n",
    "    model = load_pretrained_model(model_name)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    x_batch = main_image_batch.to(torch.uint8)\n",
    "    main_images = x_batch.numpy()\n",
    "    main_images = np.moveaxis(main_images, 1, -1)\n",
    "    main_images_prob_scores, main_pred_indices, all_blur_images_all_prob_scores, best_blur_scores, blur_pred_indices, all_images_all_relative_entropy = generate_scores_on_saliency_focused_images(model, main_images, y_batch, blurred_images_before_n_after_interp, blurred_images_rel_entropy)\n",
    "\n",
    "    print(main_images_prob_scores.shape)\n",
    "    print(all_blur_images_all_prob_scores.shape)\n",
    "    saliency_evaluation_result = {\"Main Scores\": main_images_prob_scores, \"Main Predictions\": main_pred_indices.numpy(), \"All Blur Scores\": all_blur_images_all_prob_scores, \"Best Blur Scores\": best_blur_scores, \"Blur Preds\": blur_pred_indices.numpy(), \"All Blur Entropies\": all_images_all_relative_entropy}\n",
    "    \n",
    "    np.savez(\"detail_saliency_evaluation_May2_half_percent_\"+sal_method_name+\".npz\", **saliency_evaluation_result)\n",
    "    \n",
    "    return saliency_evaluation_result\n",
    "#np.savez(\"detail_saliency_evaluation_Apr22_half_percent_\"+sal_method_name+\".npz\", **saliency_evaluation_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b66cee",
   "metadata": {},
   "source": [
    "#### **Step E:** Define functions to finalize (Interpolate/aggregate) the scores (softmax or accuracy) vs. entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interpolated_softmax_scores(all_blur_images_all_prob_scores, \\\n",
    "                                         all_images_all_relative_entropy, \n",
    "                                         main_pred_indices):\n",
    "    \n",
    "    all_common_interpolated_softmax_scores = []\n",
    "    for subj_id in range(main_pred_indices.shape[0]):\n",
    "        pred_indices=torch.LongTensor([main_pred_indices[subj_id]]*101)\n",
    "        subj_scores = all_blur_images_all_prob_scores[subj_id].gather(1, pred_indices.view(-1,1))\n",
    "        subj_scores = torch.squeeze(subj_scores)\n",
    "        original_img_score = subj_scores.clone()[-1]\n",
    "        \n",
    "        subj_scores /= original_img_score\n",
    "\n",
    "\n",
    "        rel_entropies = all_images_all_relative_entropy[subj_id].numpy()\n",
    "        \n",
    "        entropies, scores = interpolate_entropy_vs_scores(rel_entropies, subj_scores)\n",
    "        all_common_interpolated_softmax_scores.append(scores)\n",
    "        \n",
    "    all_common_interpolated_softmax_scores = np.stack(all_common_interpolated_softmax_scores, axis=0)\n",
    "    return all_common_interpolated_softmax_scores, np.median(all_common_interpolated_softmax_scores, axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "def generate_accuracy_scores(all_images_all_relative_entropy, \n",
    "                                         main_pred_indices, blur_pred_indices):\n",
    "    \n",
    "    all_common_accuracy_scores = []\n",
    "    all_entropies = []\n",
    "    for threshold_id in range(blur_pred_indices.shape[1]):\n",
    "        preds_per_info_level = blur_pred_indices[:, threshold_id]\n",
    "        accuracy = (preds_per_info_level == main_pred_indices).sum().item()\n",
    "        accuracy /= main_pred_indices.size(0)\n",
    "        all_common_accuracy_scores.append(accuracy)\n",
    "        \n",
    "        entropy = torch.mean(all_images_all_relative_entropy[:, threshold_id]).item()\n",
    "        all_entropies.append(entropy)\n",
    "    \n",
    "    entropies, all_common_interpolated_accuracy_scores = interpolate_entropy_vs_scores(all_entropies, all_common_accuracy_scores)\n",
    "    \n",
    "    return all_common_accuracy_scores, all_common_interpolated_accuracy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c05131",
   "metadata": {},
   "source": [
    "#### **Step F**: Evaluate and plot the SIC/AIC metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459f9ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pallette_1 = ['#b2182b','#ef8a62','#fddbc7','#f7f7f7','#d1e5f0','#67a9cf','#2166ac']\n",
    "pallette_2 = ['#d73027','#f46d43','#fdae61','#fee090','#e0f3f8','#abd9e9','#74add1','#4575b4']\n",
    "pallette_3 = ['#8c510a','#bf812d','#2166ac','#80cdc1','#35978f','#01665e', '#4575b4']\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.0)\n",
    "# sns.set_palette(palette=pallette_3)\n",
    "\n",
    "f_size= 18\n",
    "t_size = 24\n",
    "\n",
    "def compute_pic_and_plot(main_image_batch, save=False, filename=\"\"):   # pic: performance information curve\n",
    "    \n",
    "    legend_list = [\"Gradients\", \"IG\", \"IG_Max\", \"GDA_IG\", \"GDA_Max\", \"GGIG_IG\", \"GGIG\", \"GEG_IG\", \"GEG_Max\", \"Edge\\nDetector\", \"Random\"]\n",
    "\n",
    "\n",
    "    \n",
    "    model_metric_value = {}\n",
    "    \n",
    "    nrows, ncols = 1, 4\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize= (24, 6))\n",
    "    gs = gridspec.GridSpec(nrows, ncols,\n",
    "                       wspace=0.07, hspace=0.0)\n",
    "    \n",
    "    for col in range(ncols):\n",
    "        ax = plt.subplot(gs[0, col])\n",
    "        \n",
    "        all_method_accuracy_results = []\n",
    "        all_auc_scores = []\n",
    "        all_method_all_scores = []\n",
    "        used_legends = []\n",
    "        \n",
    "        opt_method = \"softmax\" if col < 2 else \"accuracy\"\n",
    "        model_name = \"Inception_v3\" if col % 2 == 0 else \"ResNet_101\"\n",
    "            \n",
    "        for method_id in [0, 1, 6, 9, 10]:\n",
    "\n",
    "            sal_method_name = title_set[method_id]\n",
    "            used_legends.append(legend_list[method_id])\n",
    "\n",
    "    #         saliency_evaluation_results = generate_results_dictionary(main_image_batch, sal_method_name, model_name)\n",
    "    \n",
    "            if model_name == \"ResNet_101\":\n",
    "                saliency_evaluation_results = np.load(\"./MetricEvalEntropyMaps/detail_saliency_evaluation_May2_half_percent_\"+sal_method_name+\".npz\")\n",
    "            else:\n",
    "                saliency_evaluation_results = np.load(\"./MetricEvalEntropyMaps/detail_saliency_evaluation_Apr22_half_percent_\"+sal_method_name+\".npz\")\n",
    "\n",
    "            if method_id == 0:\n",
    "                print(saliency_evaluation_results.files)\n",
    "            main_images_prob_scores = torch.from_numpy(saliency_evaluation_results['Main Scores'])\n",
    "            main_pred_indices = torch.from_numpy(saliency_evaluation_results['Main Predictions'])\n",
    "            all_blur_images_all_prob_scores = torch.from_numpy(saliency_evaluation_results['All Blur Scores'])\n",
    "            best_blur_scores = torch.from_numpy(saliency_evaluation_results['Best Blur Scores'])\n",
    "            blur_pred_indices = torch.from_numpy(saliency_evaluation_results['Blur Preds'])\n",
    "            all_images_all_relative_entropy = torch.from_numpy(saliency_evaluation_results['All Blur Entropies'])\n",
    "\n",
    "            all_blur_images_all_prob_scores = all_blur_images_all_prob_scores.reshape(200, 101, -1)\n",
    "            best_blur_scores = best_blur_scores.reshape(200, -1)\n",
    "            blur_pred_indices = blur_pred_indices.reshape(200, -1)\n",
    "            all_images_all_relative_entropy = all_images_all_relative_entropy.reshape(200, -1)\n",
    "\n",
    "            evaluation_options = {'softmax' : partial(generate_interpolated_softmax_scores, all_blur_images_all_prob_scores, all_images_all_relative_entropy, main_pred_indices), \n",
    "                            'accuracy': partial(generate_accuracy_scores, all_images_all_relative_entropy, \n",
    "                                                  main_pred_indices, blur_pred_indices)              \n",
    "            }\n",
    "\n",
    "            all_scores, normalized_scores = evaluation_options[opt_method]()\n",
    "\n",
    "            per_method_evaluation_result = {\"Normalized Scores\": np.array(normalized_scores)}\n",
    "\n",
    "#             if save:\n",
    "#                 result_save_path = \"./MetricEvalEntropyMaps/\"+model_name+\"_final_eval_result_\"+sal_method_name+\"_\"+opt_method+\".npz\"\n",
    "#                 np.savez(result_save_path, **per_method_evaluation_result)\n",
    "#                 print('Evaluation Result Saved for: {}\\t Saved Here: {}'.format(sal_method_name, result_save_path))\n",
    "\n",
    "    \n",
    "            all_method_accuracy_results.append(normalized_scores) \n",
    "            ax.plot(np.linspace(0, 1.0, 11), normalized_scores[0::10], linewidth=2.5)\n",
    "            auc = compute_auc_for_pic_metric(np.linspace(0.0, 1.0, 101), np.array(normalized_scores))\n",
    "            all_auc_scores.append(auc)\n",
    "            all_method_all_scores.append(all_scores)\n",
    "\n",
    "        all_auc_scores = np.stack(all_auc_scores, axis=0)\n",
    "        all_method_all_scores = np.stack(all_method_all_scores, axis=1)\n",
    "\n",
    "#         ax.legend(used_legends, fontsize=12, loc='upper left')\n",
    "        ax.legend(used_legends, fontsize=f_size, loc='lower center', bbox_to_anchor=(0.5, -0.55), ncol=2)\n",
    "        ax.set_xlabel(\"Normalized Estimation of Entropy\", fontsize=f_size)\n",
    "        \n",
    "        if col% 2 == 0:\n",
    "            y_label = \"Median of Normalized \"+opt_method.capitalize()+\" Score\" if opt_method == 'softmax' else opt_method.capitalize()+\" Score\"\n",
    "            \n",
    "            if col == 2:\n",
    "                t = ax.text(-0.01, 0.5, y_label, rotation=90, verticalalignment='center', horizontalalignment='right', transform=ax.transAxes, fontfamily=\"sans-serif\", fontsize=f_size)\n",
    "\n",
    "            else:\n",
    "                ax.set_ylabel(y_label, fontsize=f_size)\n",
    "                \n",
    "            title = opt_method.capitalize()+\" Information Curve (\"+opt_method[0].upper()+\"IC)\"\n",
    "            ax.set_title(title, fontsize=t_size)\n",
    "            \n",
    "        xvalues = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "        yvalues = xvalues\n",
    "\n",
    "        ax.tick_params(direction='out', pad=-2)\n",
    "        ax.set_xticks(xvalues)\n",
    "        ax.set_xticklabels(xvalues, fontsize=f_size)\n",
    "        \n",
    "        ax.set_yticks(yvalues)\n",
    "        ax.set_yticklabels(yvalues, fontsize=f_size)\n",
    "            \n",
    "#         ax.set_xticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "#         ax.set_yticklabels(fontsize=7)\n",
    "        \n",
    "        if col > 0:\n",
    "            ax.set_yticklabels([])\n",
    "            \n",
    "        model_metric_value[(model_name, opt_method)] = all_auc_scores\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    if save:\n",
    "        path = os.path.join('./Plots/Real/', filename)\n",
    "        print(path)\n",
    "        fig.savefig(path+'.svg', transparent=True, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "        fig.savefig(path+'.pdf', format='pdf', dpi=300)\n",
    "        print('Plots Saved...', path)\n",
    "        \n",
    "    plt.close(fig)\n",
    "    \n",
    "    return used_legends, model_metric_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7c83f",
   "metadata": {},
   "source": [
    "#### Evaluate softmax information curve (SIC) and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d05522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "# plt.style.use(\"dark_background\")\n",
    "sns.set(style=\"whitegrid\", font_scale=1.0)\n",
    "# used_legends, all_scores, all_auc_scores = compute_pic_and_plot(x_batch, \"softmax\", \"Resnet101\", save=True, filehint=\"ResNet101_Val_SIC\")\n",
    "used_legends, model_metric_value = compute_pic_and_plot(x_batch, save=True, filename=\"SIC_and_AIC_5\")\n",
    "print(model_metric_value)\n",
    "print(used_legends)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da744b2",
   "metadata": {},
   "source": [
    "#### Process and plot boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef514fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "\n",
    "df = prepare_dataframe_for_plot(all_scores, used_legends)\n",
    "print(df.shape)\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.0)\n",
    "f = plt.figure(figsize=(8,6))\n",
    "\n",
    "seven_class = ['#b2182b','#ef8a62','#fddbc7','#f7f7f7','#d1e5f0','#67a9cf','#2166ac']\n",
    "eight_class = ['#d73027','#f46d43','#fdae61','#fee090','#e0f3f8','#abd9e9','#74add1','#4575b4']\n",
    "\n",
    "ax= sns.boxplot(x=\"Threshold %\", y=\"Score\",linewidth=1, width=0.7, hue=\"Method\", data=df)\n",
    "ax.set_ylim([0.0, 2.5])\n",
    "ax.set_ylabel('Normalized Softmax Score')\n",
    "plt.legend(loc='upper left')\n",
    "path = os.path.join('./Plots/Real/', \"method_research_normalized_softmax_all_scores\" )\n",
    "print(path)\n",
    "f.savefig(path+'.svg', transparent=True, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "f.savefig(path+'.pdf', format='pdf', dpi=300)\n",
    "print('Plots Saved...', path)\n",
    "plt.show()\n",
    "plt.close(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1223b6",
   "metadata": {},
   "source": [
    "#### Evaluate accuracy information curve (AIC) and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d8dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use(\"dark_background\")\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "print(title_set)\n",
    "sns.set(style=\"whitegrid\", font_scale=1.0)\n",
    "used_legends, all_scores, scores = compute_pic_and_plot(\"accuracy\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55fe67",
   "metadata": {},
   "source": [
    "### Bar plots of information curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c04c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "f = plt.figure(figsize=(10,6))\n",
    "plt.bar(used_legends, scores, color ='maroon',width = 0.4)\n",
    "path = os.path.join('./Plots/Real/', \"method_research_normalized_accuracy_scores_bar\" )\n",
    "print(path)\n",
    "f.savefig(path+'.svg', transparent=True, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "f.savefig(path+'.pdf', format='pdf', dpi=300)\n",
    "print('Plots Saved...', path)\n",
    "plt.show()\n",
    "plt.close(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
