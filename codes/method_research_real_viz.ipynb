{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf77b49c",
   "metadata": {},
   "source": [
    "# CIFAR 10 Normalization Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4478daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[x/255.0 for x in [125.3, 123.0, 113.9]]\n",
    "print(mean)\n",
    "std=[x/255.0 for x in [63.0, 62.1, 66.7]]\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c42b1ec",
   "metadata": {},
   "source": [
    "# Let's see _how to load_ ``MNIST`` data\n",
    "\n",
    "- Import the **required** packages\n",
    "- Load the data \n",
    "- Do **required** normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d249d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import captum\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "\n",
    "def plot_maps_using_captum(model, FinalData, Labels, classnames, data_name, saliency_dict, method_captions, p, model_name=\"\", cm=None, vis_sign='positive'):\n",
    "    id = len(saliency_dict)\n",
    "    D = np.asarray(range(95, 105, 1))  # Using: MNIST 395 - 405, fMNIST: 10 - 20\n",
    "    L = Labels[D[:]]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(FinalData) # We don't need to unsqueeze(1) if we already have that dimension as in color image\n",
    "    \n",
    "    preds = output.data.max(1, keepdim=True)[1]\n",
    "    print(preds.shape)\n",
    "    print(torch.flatten(preds[D[:]]))\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows = 10, ncols = id+1, sharex = \"all\", figsize = (12,12), squeeze=False)\n",
    "\n",
    "    for i in range(10):\n",
    "        \n",
    "        sample = saliency_dict[0][D[i]]\n",
    "        \n",
    "        if np.squeeze(sample).ndim == 3:\n",
    "            attribution = np.transpose(sample, (1,2,0))\n",
    "        else:\n",
    "            attribution = np.expand_dims(sample, axis=2)\n",
    "        \n",
    "        img = FinalData[D[i]].numpy()\n",
    "        \n",
    "#         if np.squeeze(img).ndim == 3:\n",
    "        img = np.transpose(img, (1,2,0))\n",
    "    \n",
    "        # for imagenet\n",
    "#         mean = np.array([0.485, 0.456, 0.406])\n",
    "#         std = np.array([0.229, 0.224, 0.225])\n",
    "        \n",
    "        # for MNIST\n",
    "#         mean =np.array([0.1307])\n",
    "#         std = np.array([0.3081])\n",
    "\n",
    "        # for CIFAR 10\n",
    "    \n",
    "        mean = np.array([0.5, 0.5, 0.5])\n",
    "        std = np.array([0.5, 0.5, 0.5])\n",
    "    \n",
    "#         mean=[x/255.0 for x in [125.3, 123.0, 113.9]]\n",
    "#         std=[x/255.0 for x in [63.0, 62.1, 66.7]]\n",
    "        \n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        plt_fig, plt_axis = viz.visualize_image_attr(attribution,\n",
    "                                                     img,\n",
    "                                                     \"original_image\",\n",
    "                                                     \"all\",\n",
    "                                                     (fig.number, axes[i, 0]),\n",
    "                                                     title=\"\",\n",
    "                                                     use_pyplot = False)\n",
    "        \n",
    "        pred_title = str(classnames[preds[D[i]].item()])\n",
    "        axes[i,0].set_ylabel(\"Pr: \"+pred_title, fontsize=8)\n",
    "\n",
    "    for method_id in range(id):\n",
    "\n",
    "        saliency = saliency_dict[method_id]\n",
    "\n",
    "        for i in range(10):\n",
    "\n",
    "            sample = saliency_dict[method_id][D[i]]\n",
    "            \n",
    "            if np.squeeze(sample).ndim == 3:\n",
    "                attribution = np.transpose(sample, (1,2,0))\n",
    "            else:\n",
    "                attribution = np.expand_dims(sample, axis=2)\n",
    "            \n",
    "            fig_tuple = fig.number, axes[i, method_id + 1]\n",
    "            \n",
    "            \n",
    "            plt_fig, plt_axis = viz.visualize_image_attr(attribution,\n",
    "                                                         img,\n",
    "                                                         \"heat_map\",\n",
    "                                                         vis_sign,\n",
    "                                                         fig_tuple,\n",
    "                                                         cmap = cm,\n",
    "                                                         title=\"\",\n",
    "                                                         use_pyplot = False)\n",
    "\n",
    "    axes[0, 0].set_title('Image/Data', fontsize='medium')\n",
    "\n",
    "\n",
    "    for method_id in range(id):\n",
    "        axes[0, method_id + 1].set_title(method_captions[method_id], fontsize='medium')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    path = os.path.join('./Plots/Real/', \"method_research_\"+model_name+'_'+data_name+'_usingCapt_'+vis_sign+'_'+str(p) )\n",
    "    print(path)\n",
    "#     fig.savefig(path+'.svg', transparent=True, bbox_inches='tight', pad_inches=0)\n",
    "    fig.savefig(path+'.pdf', format='pdf', dpi=300)\n",
    "#     fig.savefig(path+'.png', format='png', dpi=300)\n",
    "#     print('Plots Saved...', path)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965d56a0",
   "metadata": {},
   "source": [
    "# Finalize MNIST Maps\n",
    "\n",
    "*  Choose appropriate *dataset* and path to *saliency*\n",
    "*  Select which methods you want to _visualize_ saliency for \n",
    "*  Load the data and saliency.\n",
    "*  Do the necessary **post-processing** for the saliency maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e5538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from methods.method_research_utilities import load_fmnist_saliency_data, generate_post_processed_maps\n",
    "# !pip install numpy==1.16.1\n",
    "%matplotlib inline\n",
    "\n",
    "method_titles = [\"GD\", \"ONLY.IG\", \"ONLY.M\", \"GDAsc.IG\",\"GDAsc.M\", \"M.GDAsc.IG\", \"M.GDAsc.M\",\"Wt.P.IG\", \"Wt.P.M\", \"IG\", \"CaptIG\", \"L.IG\"]\n",
    "\n",
    "Dataset = {0: 'mnist', 1: 'fmnist', 2: 'cifar10', 3: 'imgnet'}\n",
    "data = 0 # \n",
    "fname_common = \"method_research_\"+Dataset[data]+\"_unsoftmaxed\" #_vgg_customized\"\n",
    "# fname_common = \"method_research_\"+Dataset[data]+\"_unsoftmaxed_vgg_customized\" # for Fashion MNIST\n",
    "# fname_common = \"method_research_\"+Dataset[data]+\"_unsoftmaxed_vgg_customized_new\"\n",
    "\n",
    "# For MNIST use random_seeds = range(10)\n",
    "list_of_saliency_dicts, titles = generate_post_processed_maps(data, fname_common, method_list=[\"GD\", \"ONLY.IG\", \"ONLY.M\", \"GDAsc\", \"M.GDAsc\", \"Wt.P\"], \\\n",
    "                                                      random_seeds=list(range(10)), viz=False, scale_flag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39afda80",
   "metadata": {},
   "source": [
    "# Visualize MNIST/Fashion MNIST/CIFAR10 Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0388752",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import os\n",
    "%matplotlib inline\n",
    "from models import SanityCheckCNN, VGGNet, FashionCustomizedCNN\n",
    "\n",
    "from methods.method_research_utilities import load_mnist_saliency_data, load_fmnist_saliency_data,\\\n",
    "    load_cifar10_saliency_data, load_cifar10_original_saliency_data, load_imagenet_saliency_data\n",
    "\n",
    "FinalData, Labels, dataLoaderSal, categories = load_cifar10_original_saliency_data()\n",
    "print(FinalData.shape)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for p in range(1, 11):\n",
    "#     model = SanityCheckCNN()\n",
    "#     model = FashionCustomizedCNN()\n",
    "    model = VGGNet(3, 10) # CIFAR model\n",
    "\n",
    "#     path = './models and saliencies/method_research_mnist_model_'+str(p)+'.pth'\n",
    "    #     path = './models and saliencies/method_research_fmnist_model_vgg_customized_'+str(restart)+'.pth'\n",
    "    path = './models and saliencies/method_research_cifar_model_vgg_customized_new_'+str(p)+'.pth'\n",
    "\n",
    "    model_dict = torch.load(path, map_location=device)  # with good components\n",
    "    model.load_state_dict(model_dict)\n",
    "    model.to(device)\n",
    "    print('Model loaded from:', path)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    colors = {'positive': 'Reds', 'absolute_value': 'Reds', 'all': LinearSegmentedColormap.from_list(\"BlWhRd\", [\"blue\", \"white\",\"red\"])}\n",
    "    sign = 'absolute_value'\n",
    "\n",
    "    plot_maps_using_captum(model, FinalData, Labels, categories, Dataset[data], list_of_saliency_dicts[p-1], titles, p, cm=colors[sign], vis_sign=sign)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca9be3",
   "metadata": {},
   "source": [
    "# Visualize Fashion MNIST\n",
    "\n",
    "- Load the ``maps``\n",
    "- Preprocess them using absolute/ReLU/min-max normalization\n",
    "- Plot them **_in suitable visualization_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f7359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.method_research_utilities import load_fmnist_saliency_data, generate_post_processed_maps\n",
    "# !pip install numpy==1.16.1\n",
    "%matplotlib inline\n",
    "\n",
    "method_titles = [\"GD\", \"ONLY.IG\", \"ONLY.M\", \"GDAsc.IG\",\"GDAsc.M\", \"M.GDAsc.IG\", \"M.GDAsc.M\",\"Wt.P.IG\", \"Wt.P.M\", \"IG\", \"CaptIG\", \"L.IG\"]\n",
    "\n",
    "Dataset = {0: 'mnist', 1: 'fmnist', 2: 'cifar10', 3: 'imgnet'}\n",
    "data = 1 # \n",
    "fname_common = \"method_research_\"+Dataset[data]+\"_unsoftmaxed_vgg_customized\"\n",
    "# visualize_maps(1, fname_common, 2)\n",
    "list_of_saliency_dicts = generate_post_processed_maps(data, fname_common, random_seeds=1, viz=True, scale_flag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946431ea",
   "metadata": {},
   "source": [
    "# Finalize CIFAR 10 Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58996efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from methods.method_research_utilities import load_cifar10_saliency_data, generate_post_processed_maps\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "method_titles = [\"GD\", \"ONLY.IG\", \"ONLY.M\", \"GDAsc.IG\",\"GDAsc.M\", \"M.GDAsc.IG\", \"M.GDAsc.M\",\"Wt.P.IG\", \"Wt.P.M\", \"IG\", \"CaptIG\", \"L.IG\"]\n",
    "\n",
    "Dataset = {0: 'mnist', 1: 'fmnist', 2: 'cifar10', 3: 'imgnet'}\n",
    "data = 2 # \n",
    "fname_common = \"method_research_\"+Dataset[data]+\"_unsoftmaxed_vgg_customized_new\"\n",
    "# visualize_maps(1, fname_common, 2)\n",
    "list_of_saliency_dicts = generate_post_processed_maps(data, fname_common, method_list=[\"GD\", \"ONLY.IG\", \"ONLY.M\", \"GDAsc\", \"M.GDAsc\", \"Wt.P\"],\\\n",
    "                                                      random_seeds=list(range(1, 11)), viz=False, scale_flag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af00b1f9",
   "metadata": {},
   "source": [
    "# [Metric](www.google.com) Evaluation \n",
    "\n",
    "## ``MNIST``/``FMNIST``/``CIFAR 10`` Dataset\n",
    "[To know about all the metrics here](www.google.com)\n",
    "\n",
    "- ``sample = np.clip(sample, -1, 1)``\n",
    "- ``sample = (sample + 1.0)/2.0``\n",
    "- ``sample = sample.transpose(1,2,0)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31be3590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.method_research_utilities import load_mnist_saliency_data, load_fmnist_saliency_data, generate_post_processed_maps\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from methods.saliency_evaluation_methods import calcAllMetric, calc_metric_for_real_data\n",
    "\n",
    "def do_metric_evaluation(data):\n",
    "    \n",
    "    Dataset = {0: 'mnist', 1: 'fmnist', 2: 'cifar10', 3: 'imgnet'}\n",
    "    \n",
    "    DatasetDict = {\"mnist\": load_mnist_saliency_data, \"fmnist\": load_fmnist_saliency_data}\n",
    "    \n",
    "    # Generate/Obtain Data\n",
    "    \n",
    "    FinalData, Labels = DatasetDict[Dataset[data]]()\n",
    "    \n",
    "    print('Original Data Shape:', FinalData.shape)\n",
    "\n",
    "    print('Test Data Shape:', Labels.shape)\n",
    "    fname_common = \"method_research_\"+Dataset[data]+\"_unsoftmaxed\"  #_vgg_customized\n",
    "\n",
    "    prefix = \"method_research_\"+Dataset[data]\n",
    "\n",
    "    filenamepath = \"./models and saliencies\"\n",
    "    filenamepath = os.path.join(filenamepath, 'saliency')\n",
    "\n",
    "\n",
    "    list_of_saliency_dicts = generate_post_processed_maps(data, \\\n",
    "                                fname_common, random_seeds=1, viz=False, scale_flag=False)\n",
    "\n",
    "\n",
    "    '''\n",
    "    k: interpretability method key\n",
    "    total_labels: 10\n",
    "    '''\n",
    "    total_labels = 10\n",
    "    method_titles = [\"GD\", \"ONLY.IG\", \"ONLY.M\", \"GDAsc.IG\",\"GDAsc.M\", \"M.GDAsc.IG\", \"M.GDAsc.M\",\"Wt.P.IG\", \"Wt.P.M\"]\n",
    "\n",
    "    out_metric = {i: {k : [] for k in method_titles} for i in range(total_labels)}\n",
    "\n",
    "    for saliency_dict_item in list_of_saliency_dicts:\n",
    "        for k, v in saliency_dict_item.items():\n",
    "\n",
    "\n",
    "            # Call for metric evaluation here ....\n",
    "\n",
    "            if k < len(method_titles):\n",
    "                out = calc_metric_for_real_data(saliency_dict_item[k], FinalData, Labels, total_labels, method_titles[k])\n",
    "\n",
    "                print('Calculated Metric Shape:')\n",
    "                for lab in range(total_labels):\n",
    "                    out_metric[lab][method_titles[k]].append(np.array(out[lab]))\n",
    "\n",
    "                    print(np.array(out_metric[lab][method_titles[k]]).shape)\n",
    "\n",
    "            else:\n",
    "                print(f'Key Value: {k}, not under our consideration')\n",
    "\n",
    "\n",
    "    # Save the metrics \n",
    "\n",
    "    # np.savez(filenamepath+\"/\"+prefix+\"out_0_all_metric_ReLUed_map.npz\", **out_0_metric)  # absolute data\n",
    "    # np.savez(filenamepath+\"/\"+prefix+\"out_1_all_metric_ReLUed_map.npz\", **out_1_metric)\n",
    "\n",
    "    for labl in range(total_labels):\n",
    "        np.savez(filenamepath+\"/\"+prefix+\"_out_\"+str(labl)+\"_all_metrics_corrected_preAbsolute_map_all_same_mask.npz\", **out_metric[labl])\n",
    "\n",
    "    for k in method_titles:\n",
    "        print(f\"Method: {k}\")\n",
    "\n",
    "        for lab in range(total_labels):\n",
    "            print(np.array(out_metric[lab][k]).shape)\n",
    "            print(np.mean(out_metric[lab][k], axis=1))\n",
    "            print(np.mean(np.mean(out_metric[lab][k], axis=1), axis=0))\n",
    "    \n",
    "    print('Metric Evaluation is Done...')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8055b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_metric_evaluation(data=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c7377a",
   "metadata": {},
   "source": [
    "# Visualize Metric Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070310ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate Box Plot Using Evaluation #\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tkinter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import sys\n",
    "# from statannot import add_stat_annotation\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "\n",
    "from myscripts.rar_plot_helper import create_per_method_result\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# sns.set(style=\"darkgrid\", palette=\"pastel\", font_scale=1.9)\n",
    "sns.set(style=\"whitegrid\", font_scale=1.5)\n",
    "# sns.set_theme()\n",
    "\n",
    "\n",
    "def visualize_metric_eval_results(data, save=False):\n",
    "    \n",
    "    Dataset = {0: 'mnist', 1: 'fmnist', 2: 'cifar', 3: 'imagenet'}\n",
    "\n",
    "    class_labels = {'mnist': ['zero', 'one', 'two', 'three',\n",
    "                           'four', 'five', 'six', 'seven', 'eight', 'nine'], \\\n",
    "                   'fmnist': ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', \\\n",
    "                          'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], \\\n",
    "                   'cifar10' : ['plane', 'car', 'bird', 'cat', \\\n",
    "                           'deer', 'dog', 'frog', 'horse', 'ship', 'truck']}\n",
    "\n",
    "    classes = class_labels[Dataset[data]]\n",
    "    print(f'Classes: {classes}')\n",
    "\n",
    "\n",
    "    prefix = \"method_research_\"+Dataset[data]\n",
    "\n",
    "    filenamepath = \"./models and saliencies\"\n",
    "    filenamepath = os.path.join(filenamepath, 'saliency')\n",
    "\n",
    "    out = {}\n",
    "    npz = {}\n",
    "\n",
    "    for labl in range(10):\n",
    "        out[labl] = filenamepath+\"/\"+prefix+\"_out_\"+str(labl)+\"_all_metrics_corrected_preAbsolute_map_all_same_mask.npz\"\n",
    "        print('Loading metric results from: {}'.format(out[labl]))\n",
    "        npz[labl] = np.load(out[labl])\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    metric_names = [\"Pearson Correlation\", \"Spearman Rank\", \"Weighted Jaccard\", \"Relevance Percent\", \"Structural Similarity\", \"Mean Square Error\"]\n",
    "\n",
    "    for k in npz[0].files:\n",
    "\n",
    "        for labl in range(10):\n",
    "\n",
    "            for metric_id in range(6):\n",
    "                out_dict = {}\n",
    "                per_metric_result = npz[labl][k][:, :, metric_id]\n",
    "\n",
    "                out_dict[k] = per_metric_result.flatten()\n",
    "                df = pd.DataFrame.from_dict(out_dict)\n",
    "                df = df.rename(columns={k: \"score\"})\n",
    "                df.insert(0, \"class\", class_labels[Dataset[data]][labl])\n",
    "                df.insert(2, \"method\", k)\n",
    "                df.insert(3, \"metric\", metric_names[metric_id])\n",
    "                frames.append(df)\n",
    "        #         print(df)\n",
    "\n",
    "    my_df = pd.concat(frames, axis = 0)\n",
    "    print(my_df)\n",
    "\n",
    "    my_df.to_csv('./Experimental Results/Real/'+Dataset[data]+'_corrected_preAbsolute_maps_all_same_mask_all_metrics_results.csv', index=False)\n",
    "\n",
    "    \n",
    "    seven_class = ['#b2182b','#ef8a62','#fddbc7','#f7f7f7','#d1e5f0','#67a9cf','#2166ac']\n",
    "\n",
    "    eight_class = ['#d73027','#f46d43','#fdae61','#fee090','#e0f3f8','#abd9e9','#74add1','#4575b4']\n",
    "\n",
    "    sns.set_palette(palette=eight_class)\n",
    "\n",
    "    f, axes = plt.subplots(4, 1, figsize=(10,10))\n",
    "    \n",
    "   \n",
    "    count = 0\n",
    "    \n",
    "    print(npz[0].files)\n",
    "    \n",
    "    desired_methods = [\"GD\", \"ONLY.IG\", 'M.GDAsc.M'] #, \"Wt.P.M\"]  #'M.GDAsc.IG', \"Wt.P.IG\"\n",
    "    desired_metrics = [\"Spearman Rank\", \"Weighted Jaccard\", \"Structural Similarity\", \"Mean Square Error\"] #\"Pearson Correlation\"\n",
    "    user_legends = [\"GRAD\", \"IG\", \"GGIG\"]\n",
    "    \n",
    "    user_titles = {\"Spearman Rank\": \"Spearman Rank Correlation\", \"Weighted Jaccard\": \"Weighted Jaccard Similarity\", \n",
    "                   \"Structural Similarity\": \"Structural Similarity\", \"Mean Square Error\": \"Normalized Mean Square Error\"}\n",
    "\n",
    "    limits = {\"Pearson Correlation\": (0.3, 0.85), \"Spearman Rank\": (0.2,0.8), \n",
    "              \"Weighted Jaccard\": (0.0,0.61), \"Structural Similarity\": (0.0, 0.61), \"Mean Square Error\": (0.0, 0.81)}\n",
    "    \n",
    "    for metric in desired_metrics:\n",
    "        data_to_plot = my_df[my_df[\"metric\"].isin([metric])]\n",
    "        \n",
    "        if metric == \"Mean Square Error\":\n",
    "            dff = data_to_plot.copy()\n",
    "            m_value = dff[\"score\"].abs().max()\n",
    "            data_to_plot.loc[:, 'score'] = 1.0 - (data_to_plot.loc[:, 'score']/m_value)\n",
    "        \n",
    "        data_to_plot = data_to_plot[data_to_plot[\"method\"].isin(desired_methods)]\n",
    "        \n",
    "\n",
    "        print(data_to_plot.shape)\n",
    "        \n",
    "        print('Metric Name:{}'.format(metric))\n",
    "        print(data_to_plot.groupby(['method']).describe().score[['50%','std']])\n",
    "\n",
    "        g1 = sns.boxplot(x=\"class\", y=\"score\",linewidth=0.5, width=0.6,\n",
    "                    hue=\"method\",\n",
    "                    data=data_to_plot, ax=axes[count])\n",
    "\n",
    "        axes[count].set(ylim=limits[metric])\n",
    "        axes[count].set(yticks=np.arange(limits[metric][0], limits[metric][1], 0.2))\n",
    "        \n",
    "        axes[count].set_title(user_titles[metric], fontsize=14)\n",
    "\n",
    "        if count < 3:\n",
    "            axes[count].set_xlabel('')\n",
    "            axes[count].xaxis.set_ticklabels([])\n",
    "        else:\n",
    "            axes[count].set_xlabel('MNIST Digits', fontsize=12)\n",
    "        \n",
    "        x_ticks = axes[count].get_xticklabels()\n",
    "        labels = axes[count].yaxis.get_ticklabels()\n",
    "        \n",
    "        yticks = np.round(np.arange(limits[metric][0], limits[metric][1], 0.2), 1)\n",
    "        \n",
    "        g1.set_yticklabels(yticks, size = 15)\n",
    "            \n",
    "        axes[count].set_xticklabels(x_ticks, rotation=0, fontsize=12)\n",
    "\n",
    "        axes[count].set_ylabel('Score', fontsize=12)\n",
    "\n",
    "        handles, labels = axes[count].get_legend_handles_labels()\n",
    "        axes[count].legend(handles[:], user_legends, prop={'size': 10}, ncol=3, loc=1, title=\"\", borderaxespad=0.)\n",
    "       \n",
    "        count += 1\n",
    "\n",
    "    f.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.01, hspace=0.2)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    if save:\n",
    "        f.savefig('./Plots/Real/'+Dataset[data]+'_nips2022_corrected_preAbsolute_all_metrics_2.svg', transparent=True, bbox_inches='tight', pad_inches=0, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268acc79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_metric_eval_results(data=0, save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
